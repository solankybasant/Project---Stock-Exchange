{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCpQAmMxvPM3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from IPython.display import Image, display\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import seaborn as sn\n",
        "\n",
        "mpl.rcParams[\"figure.figsize\"] = (18,6)\n",
        "mpl.rcParams[\"axes.grid\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5-7gcvHQvPM7"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Sentiment_Analysis_Combined.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zHbRGRFdvPM8",
        "outputId": "aaab2107-8f55-41a6-c64e-32b7a21b6410"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-19c775d9-a488-428f-bbc0-b93d0dbf8e60\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19c775d9-a488-428f-bbc0-b93d0dbf8e60')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-19c775d9-a488-428f-bbc0-b93d0dbf8e60 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-19c775d9-a488-428f-bbc0-b93d0dbf8e60');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Date     Adj Close  Label          Open          High           Low  \\\n",
              "0  2008-08-08  11734.320312      1  11432.089844  11759.959961  11388.040039   \n",
              "1  2008-08-11  11782.349609      0  11729.669922  11867.110352  11675.530273   \n",
              "2  2008-08-12  11642.469727      0  11781.700195  11782.349609  11601.519531   \n",
              "3  2008-08-13  11532.959961      1  11632.809570  11633.780273  11453.339844   \n",
              "4  2008-08-14  11615.929688      1  11532.070312  11718.280273  11450.889648   \n",
              "\n",
              "      Volume  Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0  212830000      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1  183190000      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2  173590000      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3  182550000      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4  159790000      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UdocQIzEvPM-",
        "outputId": "b8dc9a94-dc07-4932-a6ef-517ba4e77605"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-34aeaaf3-107f-450a-a104-d952c0041cad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34aeaaf3-107f-450a-a104-d952c0041cad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34aeaaf3-107f-450a-a104-d952c0041cad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34aeaaf3-107f-450a-a104-d952c0041cad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Date     Adj Close  Label          Open          High           Low  \\\n",
              "0 2008-08-08  11734.320312      1  11432.089844  11759.959961  11388.040039   \n",
              "1 2008-08-11  11782.349609      0  11729.669922  11867.110352  11675.530273   \n",
              "2 2008-08-12  11642.469727      0  11781.700195  11782.349609  11601.519531   \n",
              "3 2008-08-13  11532.959961      1  11632.809570  11633.780273  11453.339844   \n",
              "4 2008-08-14  11615.929688      1  11532.070312  11718.280273  11450.889648   \n",
              "\n",
              "      Volume  Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0  212830000      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1  183190000      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2  173590000      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3  182550000      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4  159790000      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df = data.drop(columns = [\"Label\"])\n",
        "df = data\n",
        "df.Date = pd.to_datetime(df.Date, infer_datetime_format=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrtIqtlfvPNA",
        "outputId": "d84e6798-9213-4186-f23c-dca193efc576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1989 entries, 0 to 1988\n",
            "Data columns (total 13 columns):\n",
            " #   Column        Non-Null Count  Dtype         \n",
            "---  ------        --------------  -----         \n",
            " 0   Date          1989 non-null   datetime64[ns]\n",
            " 1   Adj Close     1989 non-null   float64       \n",
            " 2   Label         1989 non-null   int64         \n",
            " 3   Open          1989 non-null   float64       \n",
            " 4   High          1989 non-null   float64       \n",
            " 5   Low           1989 non-null   float64       \n",
            " 6   Volume        1989 non-null   int64         \n",
            " 7   Subjectivity  1989 non-null   float64       \n",
            " 8   Polarity      1989 non-null   float64       \n",
            " 9   compound      1989 non-null   float64       \n",
            " 10  neg           1989 non-null   float64       \n",
            " 11  pos           1989 non-null   float64       \n",
            " 12  neu           1989 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(10), int64(2)\n",
            "memory usage: 202.1 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "zmJgWOwZvPNB",
        "outputId": "e63b74bb-e4c6-4c5a-8ec7-b95c40889644"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc3b0941150>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.set_index('Date')[\"Adj Close\"].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "7rJXkQG6vPNC",
        "outputId": "29f4bc13-4fe6-4bd8-fabb-5a7334db8bf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fc3b07ebf10>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7fc3b0357410>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7fc3b038e750>],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.set_index('Date')[[\"Open\", \"High\", \"Low\"]].plot(subplots=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zCTfVrY1vPND",
        "outputId": "12a545c4-60c8-42da-8a16-97f9eafdd3aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cee376b7-773e-4755-bf0d-30dfc29e46d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cee376b7-773e-4755-bf0d-30dfc29e46d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cee376b7-773e-4755-bf0d-30dfc29e46d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cee376b7-773e-4755-bf0d-30dfc29e46d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Adj Close  Label          Open          High           Low     Volume  \\\n",
              "0  11734.320312      1  11432.089844  11759.959961  11388.040039  212830000   \n",
              "1  11782.349609      0  11729.669922  11867.110352  11675.530273  183190000   \n",
              "2  11642.469727      0  11781.700195  11782.349609  11601.519531  173590000   \n",
              "3  11532.959961      1  11632.809570  11633.780273  11453.339844  182550000   \n",
              "4  11615.929688      1  11532.070312  11718.280273  11450.889648  159790000   \n",
              "\n",
              "   Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input = df.drop(columns=[\"Date\"])\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "RoPgS2HNvPND",
        "outputId": "9bc75821-cfe2-42e3-af4a-81fe7beba60a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9a0c3140-c447-4815-993f-b15edc1d0e51\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1.989000e+03</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13463.032255</td>\n",
              "      <td>0.535445</td>\n",
              "      <td>13459.116048</td>\n",
              "      <td>13541.303173</td>\n",
              "      <td>13372.931728</td>\n",
              "      <td>1.628110e+08</td>\n",
              "      <td>0.361426</td>\n",
              "      <td>0.022722</td>\n",
              "      <td>-0.957369</td>\n",
              "      <td>0.162315</td>\n",
              "      <td>0.065675</td>\n",
              "      <td>0.772018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3144.006996</td>\n",
              "      <td>0.498867</td>\n",
              "      <td>3143.281634</td>\n",
              "      <td>3136.271725</td>\n",
              "      <td>3150.420934</td>\n",
              "      <td>9.392343e+07</td>\n",
              "      <td>0.060884</td>\n",
              "      <td>0.053687</td>\n",
              "      <td>0.199673</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>0.020968</td>\n",
              "      <td>0.041819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6547.049805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6547.009766</td>\n",
              "      <td>6709.609863</td>\n",
              "      <td>6469.950195</td>\n",
              "      <td>8.410000e+06</td>\n",
              "      <td>0.161332</td>\n",
              "      <td>-0.225978</td>\n",
              "      <td>-0.999500</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>10913.379883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10907.339844</td>\n",
              "      <td>11000.980469</td>\n",
              "      <td>10824.759766</td>\n",
              "      <td>1.000000e+08</td>\n",
              "      <td>0.321410</td>\n",
              "      <td>-0.011461</td>\n",
              "      <td>-0.996400</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.051000</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13025.580078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13022.049805</td>\n",
              "      <td>13088.110352</td>\n",
              "      <td>12953.129883</td>\n",
              "      <td>1.351700e+08</td>\n",
              "      <td>0.361652</td>\n",
              "      <td>0.024870</td>\n",
              "      <td>-0.993200</td>\n",
              "      <td>0.159000</td>\n",
              "      <td>0.064000</td>\n",
              "      <td>0.773000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>16478.410156</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16477.699219</td>\n",
              "      <td>16550.070312</td>\n",
              "      <td>16392.769531</td>\n",
              "      <td>1.926000e+08</td>\n",
              "      <td>0.400533</td>\n",
              "      <td>0.057980</td>\n",
              "      <td>-0.985500</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.079000</td>\n",
              "      <td>0.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>18312.390625</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18315.060547</td>\n",
              "      <td>18351.359375</td>\n",
              "      <td>18272.560547</td>\n",
              "      <td>6.749200e+08</td>\n",
              "      <td>0.615242</td>\n",
              "      <td>0.195774</td>\n",
              "      <td>0.991700</td>\n",
              "      <td>0.316000</td>\n",
              "      <td>0.153000</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a0c3140-c447-4815-993f-b15edc1d0e51')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a0c3140-c447-4815-993f-b15edc1d0e51 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a0c3140-c447-4815-993f-b15edc1d0e51');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          Adj Close        Label          Open          High           Low  \\\n",
              "count   1989.000000  1989.000000   1989.000000   1989.000000   1989.000000   \n",
              "mean   13463.032255     0.535445  13459.116048  13541.303173  13372.931728   \n",
              "std     3144.006996     0.498867   3143.281634   3136.271725   3150.420934   \n",
              "min     6547.049805     0.000000   6547.009766   6709.609863   6469.950195   \n",
              "25%    10913.379883     0.000000  10907.339844  11000.980469  10824.759766   \n",
              "50%    13025.580078     1.000000  13022.049805  13088.110352  12953.129883   \n",
              "75%    16478.410156     1.000000  16477.699219  16550.070312  16392.769531   \n",
              "max    18312.390625     1.000000  18315.060547  18351.359375  18272.560547   \n",
              "\n",
              "             Volume  Subjectivity     Polarity     compound          neg  \\\n",
              "count  1.989000e+03   1989.000000  1989.000000  1989.000000  1989.000000   \n",
              "mean   1.628110e+08      0.361426     0.022722    -0.957369     0.162315   \n",
              "std    9.392343e+07      0.060884     0.053687     0.199673     0.038575   \n",
              "min    8.410000e+06      0.161332    -0.225978    -0.999500     0.059000   \n",
              "25%    1.000000e+08      0.321410    -0.011461    -0.996400     0.135000   \n",
              "50%    1.351700e+08      0.361652     0.024870    -0.993200     0.159000   \n",
              "75%    1.926000e+08      0.400533     0.057980    -0.985500     0.188000   \n",
              "max    6.749200e+08      0.615242     0.195774     0.991700     0.316000   \n",
              "\n",
              "               pos          neu  \n",
              "count  1989.000000  1989.000000  \n",
              "mean      0.065675     0.772018  \n",
              "std       0.020968     0.041819  \n",
              "min       0.007000     0.588000  \n",
              "25%       0.051000     0.746000  \n",
              "50%       0.064000     0.773000  \n",
              "75%       0.079000     0.802000  \n",
              "max       0.153000     0.894000  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FboEhtmXvPNF"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qj4LiprVvPNF"
      },
      "outputs": [],
      "source": [
        "features = df_scaled\n",
        "target = df_scaled[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IthSpTxmvPNG"
      },
      "source": [
        "<center>features is a 2D list, target is a 1D list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yGt_aaKFvPNI"
      },
      "outputs": [],
      "source": [
        "def plotHist(history : tf.keras.callbacks.History, val : str):\n",
        "    plt.plot(history.history[val])\n",
        "    plt.plot(history.history[\"val_\" + val])\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['train', 'test'], loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rbfF1h_vPNJ",
        "outputId": "26da3563-da50-4b0a-c5ac-c743fe51b969"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1989, 12, 1989)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(features), len(features[0]), len(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Gb3S1utbvPNK",
        "outputId": "aad81e6f-8e78-439d-be2b-e4869d6fa48c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6d82bd48-2629-41f6-878b-f9531be57fcc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d82bd48-2629-41f6-878b-f9531be57fcc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d82bd48-2629-41f6-878b-f9531be57fcc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d82bd48-2629-41f6-878b-f9531be57fcc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Adj Close  Label          Open          High           Low     Volume  \\\n",
              "0  11734.320312      1  11432.089844  11759.959961  11388.040039  212830000   \n",
              "1  11782.349609      0  11729.669922  11867.110352  11675.530273  183190000   \n",
              "2  11642.469727      0  11781.700195  11782.349609  11601.519531  173590000   \n",
              "3  11532.959961      1  11632.809570  11633.780273  11453.339844  182550000   \n",
              "4  11615.929688      1  11532.070312  11718.280273  11450.889648  159790000   \n",
              "\n",
              "   Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RapvY6CCvPNL",
        "outputId": "32d0f01b-da52-4757-b533-9a7cf4c81a57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4.40894198e-01, 1.00000000e+00, 4.15113783e-01, 4.33813671e-01,\n",
              "       4.16695095e-01, 3.06702075e-01, 2.34004623e-01, 4.20649400e-01,\n",
              "       6.52872640e-04, 6.84824903e-01, 2.32876712e-01, 4.44444444e-01])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQGtPg2AvPNM",
        "outputId": "35bb301c-af20-41ec-aa12-10b249c55066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lOiZMJPmvPNM"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81gTm2CVvPNN",
        "outputId": "e10bdb99-f2c8-4a45-889a-0f27a423657a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1491, 12), (498, 12), (1491,), (498,))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BruWh_GZvPNO"
      },
      "outputs": [],
      "source": [
        "win_len = 30\n",
        "batch_size = 32\n",
        "num_features = 12\n",
        "train_generator = TimeseriesGenerator(x_train, y_train, length=win_len, sampling_rate=1, batch_size=batch_size)\n",
        "test_generator = TimeseriesGenerator(x_test, y_test, length=win_len, sampling_rate=1, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCQdKT5MvPNP",
        "outputId": "68fda896-a6ea-48c5-e8af-aca8d0850162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of batches in training set : 46\n",
            "number of batches in testing set : 15\n",
            "batch size : 32\n",
            "window size : 30\n",
            "number of features : 12\n"
          ]
        }
      ],
      "source": [
        "print(\"number of batches in training set :\", len(train_generator))\n",
        "print(\"number of batches in testing set :\", len(test_generator))\n",
        "print(\"batch size :\", len(train_generator[0][0]))\n",
        "print(\"window size :\", len(train_generator[0][0][0]))\n",
        "print(\"number of features :\", len(train_generator[0][0][0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJW5vQmvPNQ"
      },
      "source": [
        "<center><h2>Making the Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1IWbG_TsvPNR"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = tf.keras.layers.Dense(1)(x)\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMIb8RBxvPNT"
      },
      "source": [
        "<center><h3>Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l1DsknvhvPNT"
      },
      "outputs": [],
      "source": [
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model1 = build_model(input_shape, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], \n",
        "                    mlp_dropout=0.4,dropout=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1pvIbO6vPNU",
        "outputId": "d4ea1c0f-93a8-4d09-9f58-6af9e4b8e230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 30, 12)      24          ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 30, 12)      52236       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 30, 12)       0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 30, 12)      0           ['dropout[0][0]',                \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 30, 4)        52          ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 30, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 30, 12)       60          ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 30, 12)      0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 30, 12)      52236       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 30, 12)      0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 30, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 30, 12)       60          ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 30, 12)      0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 30, 12)      52236       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 30, 12)      0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 30, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 30, 12)       60          ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 30, 12)      0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 30, 12)      52236       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 30, 12)      0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 30, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 30, 12)       60          ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 30, 12)      0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 30)          0           ['tf.__operators__.add_7[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          3968        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 213,681\n",
            "Trainable params: 213,681\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4bdzkGBDvPNU"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model1.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model1.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjPZgn9DvPNV",
        "outputId": "122820bb-5ac7-423b-8e90-e4ec1a0251ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.5816 - accuracy: 0.5195\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51496, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 13s 37ms/step - loss: 2.5816 - accuracy: 0.5195 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 2/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 3.4910 - accuracy: 0.5114\n",
            "Epoch 2: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.4291 - accuracy: 0.5051 - val_loss: 1.1387 - val_accuracy: 0.4850\n",
            "Epoch 3/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.1076 - accuracy: 0.5120\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 2.1076 - accuracy: 0.5120 - val_loss: 0.8894 - val_accuracy: 0.4850\n",
            "Epoch 4/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.2054 - accuracy: 0.5083\n",
            "Epoch 4: val_accuracy improved from 0.51496 to 0.52137, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 1.2224 - accuracy: 0.5065 - val_loss: 0.6955 - val_accuracy: 0.5214\n",
            "Epoch 5/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 1.1576 - accuracy: 0.5014\n",
            "Epoch 5: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 1.1542 - accuracy: 0.5010 - val_loss: 0.7110 - val_accuracy: 0.4850\n",
            "Epoch 6/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 5.8081 - accuracy: 0.4805\n",
            "Epoch 6: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 5.8081 - accuracy: 0.4805 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 7/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3410 - accuracy: 0.4593\n",
            "Epoch 7: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3410 - accuracy: 0.4593 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 8/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 8: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 9/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 9: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 10/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 10: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 11/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 11: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 12/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3134 - accuracy: 0.4604\n",
            "Epoch 12: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3417 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 13/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 13: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 14/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3518 - accuracy: 0.4586\n",
            "Epoch 14: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3518 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 15/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3419 - accuracy: 0.4586\n",
            "Epoch 15: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3419 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 16/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 16: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 17/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 17: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 18/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 18: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 19/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2413 - accuracy: 0.4638\n",
            "Epoch 19: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3224 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 20/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 20: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 21/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2714 - accuracy: 0.4638\n",
            "Epoch 21: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3515 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 22/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 22: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 23/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3338 - accuracy: 0.4586\n",
            "Epoch 23: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3338 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 24/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.7231 - accuracy: 0.4737\n",
            "Epoch 24: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.6517 - accuracy: 0.4757 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 25/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.7549 - accuracy: 0.5291\n",
            "Epoch 25: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 5.6460 - accuracy: 0.5291 - val_loss: 1.0194 - val_accuracy: 0.4850\n",
            "Epoch 26/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 4.0451 - accuracy: 0.5064\n",
            "Epoch 26: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.9964 - accuracy: 0.5003 - val_loss: 0.8394 - val_accuracy: 0.4850\n",
            "Epoch 27/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 3.0296 - accuracy: 0.5085\n",
            "Epoch 27: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.0148 - accuracy: 0.5086 - val_loss: 0.6976 - val_accuracy: 0.4679\n",
            "Epoch 28/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 4.7375 - accuracy: 0.5305\n",
            "Epoch 28: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 4.7375 - accuracy: 0.5305 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.7717 - accuracy: 0.5384\n",
            "Epoch 29: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.6398 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8843 - accuracy: 0.5181\n",
            "Epoch 30: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8481 - accuracy: 0.5209 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8596 - accuracy: 0.5359\n",
            "Epoch 31: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8596 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8379 - accuracy: 0.5270\n",
            "Epoch 32: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8379 - accuracy: 0.5270 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1121 - accuracy: 0.5241\n",
            "Epoch 33: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.0315 - accuracy: 0.5298 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0665 - accuracy: 0.5334\n",
            "Epoch 34: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9876 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.9813 - accuracy: 0.5398\n",
            "Epoch 35: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9054 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9387 - accuracy: 0.5400\n",
            "Epoch 36: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9387 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9395 - accuracy: 0.5414\n",
            "Epoch 37: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9395 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0122 - accuracy: 0.5396\n",
            "Epoch 38: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9845 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9727 - accuracy: 0.5403\n",
            "Epoch 39: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9353 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9897 - accuracy: 0.5368\n",
            "Epoch 40: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9519 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8219 - accuracy: 0.5400\n",
            "Epoch 41: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8219 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9316 - accuracy: 0.5414\n",
            "Epoch 42: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9316 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8715 - accuracy: 0.5448\n",
            "Epoch 43: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8715 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9221 - accuracy: 0.5359\n",
            "Epoch 44: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9221 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.4088 - accuracy: 0.5192\n",
            "Epoch 45: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.3480 - accuracy: 0.5243 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.4913 - accuracy: 0.5327\n",
            "Epoch 46: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 5.4032 - accuracy: 0.5346 - val_loss: 0.7622 - val_accuracy: 0.4850\n",
            "Epoch 47/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.1726 - accuracy: 0.4860\n",
            "Epoch 47: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.1726 - accuracy: 0.4860 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.3606 - accuracy: 0.5340\n",
            "Epoch 48: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.3424 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.4795 - accuracy: 0.5410\n",
            "Epoch 49: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.4821 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8096 - accuracy: 0.5347\n",
            "Epoch 50: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.7851 - accuracy: 0.5366 - val_loss: 6.5764 - val_accuracy: 0.5150\n",
            "Epoch 51/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8189 - accuracy: 0.5373\n",
            "Epoch 51: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8189 - accuracy: 0.5373 - val_loss: 1.1434 - val_accuracy: 0.4808\n",
            "Epoch 52/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8701 - accuracy: 0.5375\n",
            "Epoch 52: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8444 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9944 - accuracy: 0.5366\n",
            "Epoch 53: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9944 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.9508 - accuracy: 0.5384\n",
            "Epoch 54: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8657 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9967 - accuracy: 0.5366\n",
            "Epoch 55: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9967 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0001 - accuracy: 0.5382\n",
            "Epoch 56: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9725 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9012 - accuracy: 0.5428\n",
            "Epoch 57: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9012 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9325 - accuracy: 0.5387\n",
            "Epoch 58: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9325 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8728 - accuracy: 0.5441\n",
            "Epoch 59: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8728 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9176 - accuracy: 0.5410\n",
            "Epoch 60: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8912 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0570 - accuracy: 0.5311\n",
            "Epoch 61: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0570 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9197 - accuracy: 0.5421\n",
            "Epoch 62: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9197 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9445 - accuracy: 0.5407\n",
            "Epoch 63: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9445 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9728 - accuracy: 0.5428\n",
            "Epoch 64: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9728 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 65: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 66/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 66: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0724 - accuracy: 0.5362\n",
            "Epoch 67: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 68: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 69: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9831 - accuracy: 0.5421\n",
            "Epoch 70: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9831 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0038 - accuracy: 0.5407\n",
            "Epoch 71: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0038 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 72: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9786 - accuracy: 0.5424\n",
            "Epoch 73: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9514 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9833 - accuracy: 0.5414\n",
            "Epoch 74: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9833 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0104 - accuracy: 0.5403\n",
            "Epoch 75: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0144 - accuracy: 0.5400\n",
            "Epoch 76: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.0144 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 77: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0738 - accuracy: 0.5348\n",
            "Epoch 78: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9947 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9845 - accuracy: 0.5414\n",
            "Epoch 79: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9845 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9933 - accuracy: 0.5414\n",
            "Epoch 80: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9933 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9940 - accuracy: 0.5407\n",
            "Epoch 81: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9940 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 82: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9937 - accuracy: 0.5414\n",
            "Epoch 83: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9937 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0129 - accuracy: 0.5389\n",
            "Epoch 84: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9852 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9737 - accuracy: 0.5414\n",
            "Epoch 85: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9737 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0216 - accuracy: 0.5389\n",
            "Epoch 86: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9937 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0725 - accuracy: 0.5362\n",
            "Epoch 87: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 88: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 89: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0844 - accuracy: 0.5348\n",
            "Epoch 90: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0048 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 91: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0104 - accuracy: 0.5403\n",
            "Epoch 92: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9829 - accuracy: 0.5421\n",
            "Epoch 93: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0836 - accuracy: 0.5355\n",
            "Epoch 94: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.0041 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 95: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9843 - accuracy: 0.5414\n",
            "Epoch 96: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9843 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0506 - accuracy: 0.5376\n",
            "Epoch 97: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9723 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9726 - accuracy: 0.5428\n",
            "Epoch 98: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9726 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0511 - accuracy: 0.5376\n",
            "Epoch 99: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9623 - accuracy: 0.5435 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0222 - accuracy: 0.5389\n",
            "Epoch 100: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9943 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0001 - accuracy: 0.5394\n",
            "Epoch 101: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0001 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0672 - accuracy: 0.5154\n",
            "Epoch 102: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0672 - accuracy: 0.5154 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 103/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.3525 - accuracy: 0.4979\n",
            "Epoch 103: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.3199 - accuracy: 0.5003 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0709 - accuracy: 0.5352\n",
            "Epoch 104: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0709 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.7310 - accuracy: 0.5469\n",
            "Epoch 105: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.7542 - accuracy: 0.5428 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 106/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8774 - accuracy: 0.5380\n",
            "Epoch 106: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8774 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 107: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 108: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 109: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 110: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 111: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 113: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 114: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 116: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 117: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 118: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 119: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 120: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 121: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 122: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 123: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 124: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 125: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 126: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 127: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 128: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 129: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 130: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 131: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 133: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 134: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 135: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 136: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 137: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 138: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 139: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 140: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 142: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 143: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 144: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 145: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 146: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 147: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 148: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 150: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 151/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 151: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 152/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 152: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 153/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 153: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 154/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 154: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 155/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 155: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 156/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 156: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 157/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 157: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 158/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 158: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 159/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 159: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 160/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 160: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 161/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 161: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 162/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 162: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 163/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 163: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 164/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 164: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 165/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 165: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 166/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 166: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 167/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 167: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 168/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 168: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 169/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 169: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 170/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 170: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 171/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 171: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 172/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 172: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 173/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 173: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 174/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 174: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 175/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 175: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 176/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 176: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 177/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 177: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 178/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 178: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 179/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 179: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 180/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 180: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 181/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 181: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 182/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 182: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 183/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 183: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 184/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 184: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 185/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 185: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 186/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 186: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 187/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 187: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 188/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 188: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 189/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 189: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 190/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 190: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 191/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 191: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 192/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 192: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 193/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 193: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 194/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 194: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 195/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 195: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 196/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 196: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 197/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 197: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 198/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 198: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 199/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 199: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 200/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 200: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 201/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 201: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 202/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 202: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 203/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 203: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 204/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 204: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 205/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 205: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 206/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 206: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 207/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 207: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 208/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 208: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 209/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 209: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 210/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 210: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 211/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 211: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 212/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 212: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 213/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 213: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 214/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 214: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 215/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 215: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 216/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 216: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 217/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 217: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 218/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 218: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 219/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 219: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 220/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 220: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 221/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 221: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 222/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 222: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 223/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 223: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 224/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 224: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 225/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 225: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 226/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 226: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 227/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 227: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 228/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 228: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 229/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 229: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 230/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 230: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 231/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 231: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 232/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 232: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 233/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 233: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 234/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 234: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 235/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 235: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 236/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 236: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 237/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 237: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 238/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 238: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 239/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 239: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 240/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 240: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 241/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 241: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 242/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 242: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 243/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 243: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 244/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 244: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 245/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 245: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 246/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 246: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 247/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 247: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 248/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 248: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 249/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 249: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 250/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 250: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 251/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 251: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 252/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 252: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 253/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 253: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 254/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 254: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 255/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 255: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 256/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 256: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 257/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 257: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 258/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 258: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 259/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 259: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 260/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 260: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 261/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 261: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 262/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 262: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 263/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 263: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 264/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 264: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 265/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 265: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 266/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 266: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 267/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 267: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 268/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 268: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 269/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 269: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 270/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 270: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 271/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 271: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 272/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 272: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 273/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 273: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 274/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 274: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 275/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 275: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 276/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 276: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 277/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 277: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 278/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 278: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 279/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 279: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 280/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 280: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 281/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 281: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 282/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 282: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 283/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 283: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 284/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 284: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 285/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 285: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 286/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 286: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 287/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 287: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 288/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 288: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 289/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 289: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 290/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 290: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 291/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 291: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 292/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 292: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 48ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 293/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 293: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 294/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 294: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 295/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 295: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 296/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 296: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 297/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 297: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 298/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 298: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 299/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 299: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 300/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 300: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 301/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 301: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 302/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 302: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 303/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 303: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 304/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 304: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 305/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 305: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 306/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 306: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 307/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 307: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 308/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 308: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 309/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 309: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 310/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 310: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 311/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 311: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 312/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 312: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 313/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 313: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 314/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 314: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 315/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 315: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 316/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 316: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 317/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 317: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 318/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 318: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 319/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 319: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 320/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 320: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 321/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 321: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 322/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 322: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 323/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 323: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 324/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 324: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 325/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 325: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 326/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 326: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 327/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 327: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 328/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 328: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 329/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 329: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 330/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 330: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 331/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 331: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 332/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 332: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 333/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 333: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 334/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 334: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 335/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 335: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 336/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 336: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 337/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 337: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 338/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 338: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 339/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 339: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 340/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 340: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 341/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 341: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 342/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 342: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 343/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 343: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 344/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 344: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 345/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 345: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 346/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 346: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 347/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 347: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 348/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 348: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 349/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 349: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 350/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 350: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 351/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 351: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 352/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 352: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 353/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 353: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 354/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 354: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 355/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 355: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 356/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 356: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 357/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 357: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 358/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 358: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 359/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 359: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 360/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 360: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 361/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 361: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 362/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 362: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 363/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 363: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 364/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 364: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 365/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 365: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 366/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 366: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 367/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 367: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 368/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 368: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 369/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 369: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 370/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 370: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 371/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 371: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 372/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 372: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 373/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 373: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 374/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 374: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 375/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 375: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 376/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 376: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 377/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 377: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 378/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 378: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 379/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 379: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 380/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 380: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 381/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 381: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 382/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 382: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 383/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 383: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 384/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 384: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 385/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 385: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 386/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 386: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 387/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 387: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 388/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 388: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 389/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 389: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 390/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 390: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 391/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 391: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 392/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 392: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 393/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 393: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 394/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 394: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 395/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 395: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 396/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 396: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 397/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 397: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 398/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 398: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 399/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 399: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 400/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 400: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 401/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 401: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 402/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 402: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 403/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 403: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 404/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 404: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 405/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 405: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 406/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 406: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 407/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 407: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 408/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 408: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 409/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 409: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 410/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 410: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 411/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 411: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 412/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 412: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 413/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 413: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 414/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 414: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 415/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 415: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 416/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 416: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 417/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 417: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 418/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 418: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 419/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 419: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 420/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 420: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 421/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 421: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 422/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 422: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 423/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 423: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 424/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 424: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 425/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 425: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 426/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 426: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 427/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 427: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 428/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 428: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 429/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 429: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 430/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 430: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 431/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 431: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 432/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 432: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 433/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 433: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 434/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 434: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 435/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 435: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 436/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 436: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 437/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 437: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 438/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 438: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 439/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 439: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 440/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 440: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 441/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 441: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 442/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 442: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 443/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 443: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 444/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 444: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 445/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 445: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 446/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 446: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 447/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 447: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 448/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 448: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 449/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 449: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 450/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 450: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 451/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 451: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 452/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 452: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 453/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 453: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 454/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 454: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 455/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 455: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 456/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 456: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 457/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 457: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 458/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 458: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 459/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 459: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 460/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 460: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 461/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 461: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 462/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 462: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 463/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 463: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 464/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 464: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 465/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 465: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 466/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 466: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 467/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 467: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 468/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 468: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 469/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 469: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 3s 54ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 470/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 470: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 471/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 471: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 472/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 472: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 473/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 473: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 474/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 474: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 475/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 475: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 476/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 476: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 477/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 477: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 478/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 478: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 479/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 479: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 480/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 480: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 481/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 481: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 482/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 482: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 483/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 483: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 484/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 484: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 485/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 485: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 486/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 486: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 487/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 487: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 488/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 488: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 489/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 489: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 490/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 490: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 491/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 491: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 492/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 492: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 493/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 493: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 494/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 494: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 495/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 495: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 496/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 496: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 497/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 497: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 498/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 498: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 499/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 499: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 500/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 500: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "# history = model1.fit()\n",
        "history = model1.fit_generator(train_generator, epochs=500, validation_data=test_generator, shuffle=False, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "EDGPbMZ7vPNW",
        "outputId": "4d9b4038-22b4-4c61-e080-118737126750"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plotHist(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "0xfxCV-9vPNX",
        "outputId": "49c39539-1068-4780-b7df-e34ea352b485"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plotHist(history, \"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CJd7-j2TvPNY"
      },
      "outputs": [],
      "source": [
        "model1 = tf.keras.models.load_model(\"/content/clas_logs\\model1.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr33w9zBvPNY",
        "outputId": "b4cbc2b4-7c6e-46c0-c28a-5afa3f764c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model1.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMnQcuHMvPNZ",
        "outputId": "5ac4e8cf-bf92-40d8-dee8-f1888f21d24e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.49025583],\n",
              "       [0.5691159 ],\n",
              "       [0.47406   ],\n",
              "       [0.5144795 ],\n",
              "       [0.5117658 ],\n",
              "       [0.54769695],\n",
              "       [0.52417076],\n",
              "       [0.48997888],\n",
              "       [0.50442535],\n",
              "       [0.5678146 ],\n",
              "       [0.508131  ],\n",
              "       [0.5509618 ],\n",
              "       [0.5307672 ],\n",
              "       [0.47099188],\n",
              "       [0.56188166],\n",
              "       [0.5139838 ],\n",
              "       [0.5125375 ],\n",
              "       [0.5360035 ],\n",
              "       [0.54639846],\n",
              "       [0.5275662 ],\n",
              "       [0.52931356],\n",
              "       [0.5235117 ],\n",
              "       [0.5445008 ],\n",
              "       [0.47072476],\n",
              "       [0.4907159 ],\n",
              "       [0.4919728 ],\n",
              "       [0.4609087 ],\n",
              "       [0.5047648 ],\n",
              "       [0.41215336],\n",
              "       [0.4595322 ],\n",
              "       [0.49176407],\n",
              "       [0.4710005 ],\n",
              "       [0.47615796],\n",
              "       [0.49271333],\n",
              "       [0.4621929 ],\n",
              "       [0.5183671 ],\n",
              "       [0.4847251 ],\n",
              "       [0.5145487 ],\n",
              "       [0.4524294 ],\n",
              "       [0.48918787],\n",
              "       [0.47088045],\n",
              "       [0.44183606],\n",
              "       [0.49572098],\n",
              "       [0.48349988],\n",
              "       [0.48515478],\n",
              "       [0.49949846],\n",
              "       [0.4909861 ],\n",
              "       [0.50841093],\n",
              "       [0.471227  ],\n",
              "       [0.43343163],\n",
              "       [0.48105636],\n",
              "       [0.4706308 ],\n",
              "       [0.49330437],\n",
              "       [0.48821157],\n",
              "       [0.47890466],\n",
              "       [0.48380595],\n",
              "       [0.49999034],\n",
              "       [0.47614962],\n",
              "       [0.51162934],\n",
              "       [0.5011924 ],\n",
              "       [0.5278441 ],\n",
              "       [0.539327  ],\n",
              "       [0.4875705 ],\n",
              "       [0.5640831 ],\n",
              "       [0.50629103],\n",
              "       [0.48040214],\n",
              "       [0.5184918 ],\n",
              "       [0.52165425],\n",
              "       [0.50833404],\n",
              "       [0.51469076],\n",
              "       [0.46887332],\n",
              "       [0.5061791 ],\n",
              "       [0.47625947],\n",
              "       [0.548028  ],\n",
              "       [0.47382557],\n",
              "       [0.47935188],\n",
              "       [0.50587744],\n",
              "       [0.4326314 ],\n",
              "       [0.4478628 ],\n",
              "       [0.46500868],\n",
              "       [0.44853905],\n",
              "       [0.47603482],\n",
              "       [0.4644961 ],\n",
              "       [0.5023848 ],\n",
              "       [0.5189267 ],\n",
              "       [0.5042836 ],\n",
              "       [0.511239  ],\n",
              "       [0.4391733 ],\n",
              "       [0.5130458 ],\n",
              "       [0.469251  ],\n",
              "       [0.41829696],\n",
              "       [0.46324462],\n",
              "       [0.4052632 ],\n",
              "       [0.41996473],\n",
              "       [0.41477752],\n",
              "       [0.4129193 ],\n",
              "       [0.42499754],\n",
              "       [0.47236103],\n",
              "       [0.47061515],\n",
              "       [0.5046241 ],\n",
              "       [0.43216324],\n",
              "       [0.49535352],\n",
              "       [0.4613232 ],\n",
              "       [0.44820553],\n",
              "       [0.5118057 ],\n",
              "       [0.509043  ],\n",
              "       [0.49040186],\n",
              "       [0.43171608],\n",
              "       [0.42400324],\n",
              "       [0.4747743 ],\n",
              "       [0.42430556],\n",
              "       [0.44111302],\n",
              "       [0.4359442 ],\n",
              "       [0.43055546],\n",
              "       [0.50220555],\n",
              "       [0.51203173],\n",
              "       [0.4877808 ],\n",
              "       [0.47880712],\n",
              "       [0.4257512 ],\n",
              "       [0.5083858 ],\n",
              "       [0.44327673],\n",
              "       [0.48032796],\n",
              "       [0.4831094 ],\n",
              "       [0.48739132],\n",
              "       [0.5145298 ],\n",
              "       [0.42447197],\n",
              "       [0.46970126],\n",
              "       [0.50165915],\n",
              "       [0.414088  ],\n",
              "       [0.47773454],\n",
              "       [0.44020557],\n",
              "       [0.42212212],\n",
              "       [0.5103651 ],\n",
              "       [0.44916356],\n",
              "       [0.4729231 ],\n",
              "       [0.46254963],\n",
              "       [0.39785182],\n",
              "       [0.48194647],\n",
              "       [0.4303987 ],\n",
              "       [0.42474723],\n",
              "       [0.4531993 ],\n",
              "       [0.46459532],\n",
              "       [0.48048025],\n",
              "       [0.40898538],\n",
              "       [0.4494243 ],\n",
              "       [0.4570004 ],\n",
              "       [0.40754426],\n",
              "       [0.44853753],\n",
              "       [0.43183503],\n",
              "       [0.37690598],\n",
              "       [0.4145599 ],\n",
              "       [0.40958577],\n",
              "       [0.41043743],\n",
              "       [0.44239843],\n",
              "       [0.39840332],\n",
              "       [0.4705405 ],\n",
              "       [0.4553632 ],\n",
              "       [0.42459017],\n",
              "       [0.46286997],\n",
              "       [0.44890147],\n",
              "       [0.46323884],\n",
              "       [0.45364386],\n",
              "       [0.45954224],\n",
              "       [0.4699266 ],\n",
              "       [0.45811483],\n",
              "       [0.41524765],\n",
              "       [0.43134773],\n",
              "       [0.42843252],\n",
              "       [0.46455   ],\n",
              "       [0.43788135],\n",
              "       [0.47981644],\n",
              "       [0.4477476 ],\n",
              "       [0.47481507],\n",
              "       [0.48035648],\n",
              "       [0.46816754],\n",
              "       [0.4475217 ],\n",
              "       [0.4509954 ],\n",
              "       [0.46429837],\n",
              "       [0.47047073],\n",
              "       [0.46176502],\n",
              "       [0.4634427 ],\n",
              "       [0.4946766 ],\n",
              "       [0.46162227],\n",
              "       [0.40318638],\n",
              "       [0.47435862],\n",
              "       [0.51038   ],\n",
              "       [0.4739139 ],\n",
              "       [0.46767557],\n",
              "       [0.49420825],\n",
              "       [0.44777083],\n",
              "       [0.46735236],\n",
              "       [0.44472012],\n",
              "       [0.44027412],\n",
              "       [0.4175495 ],\n",
              "       [0.39641085],\n",
              "       [0.46201444],\n",
              "       [0.41921702],\n",
              "       [0.45297903],\n",
              "       [0.4868019 ],\n",
              "       [0.39444003],\n",
              "       [0.43786305],\n",
              "       [0.44608548],\n",
              "       [0.47423214],\n",
              "       [0.47458807],\n",
              "       [0.4071982 ],\n",
              "       [0.4818026 ],\n",
              "       [0.4442066 ],\n",
              "       [0.43717203],\n",
              "       [0.4819438 ],\n",
              "       [0.35734585],\n",
              "       [0.4258217 ],\n",
              "       [0.4408118 ],\n",
              "       [0.4099563 ],\n",
              "       [0.42363015],\n",
              "       [0.41134492],\n",
              "       [0.4161293 ],\n",
              "       [0.40313873],\n",
              "       [0.40050092],\n",
              "       [0.43055886],\n",
              "       [0.3993435 ],\n",
              "       [0.40817082],\n",
              "       [0.4430823 ],\n",
              "       [0.4202834 ],\n",
              "       [0.4629374 ],\n",
              "       [0.432047  ],\n",
              "       [0.4577408 ],\n",
              "       [0.4136807 ],\n",
              "       [0.46024245],\n",
              "       [0.42979017],\n",
              "       [0.45067957],\n",
              "       [0.4049615 ],\n",
              "       [0.33714387],\n",
              "       [0.40043503],\n",
              "       [0.42847353],\n",
              "       [0.39011127],\n",
              "       [0.42964408],\n",
              "       [0.45895216],\n",
              "       [0.41334754],\n",
              "       [0.43249092],\n",
              "       [0.40516922],\n",
              "       [0.4714658 ],\n",
              "       [0.43636286],\n",
              "       [0.45144856],\n",
              "       [0.4402122 ],\n",
              "       [0.40862906],\n",
              "       [0.42947224],\n",
              "       [0.40873665],\n",
              "       [0.4260269 ],\n",
              "       [0.444938  ],\n",
              "       [0.44733503],\n",
              "       [0.40538266],\n",
              "       [0.41583198],\n",
              "       [0.3907553 ],\n",
              "       [0.42081153],\n",
              "       [0.43076825],\n",
              "       [0.42024368],\n",
              "       [0.3944985 ],\n",
              "       [0.45697483],\n",
              "       [0.42376605],\n",
              "       [0.41795403],\n",
              "       [0.4565965 ],\n",
              "       [0.44914192],\n",
              "       [0.512779  ],\n",
              "       [0.43648112],\n",
              "       [0.45449767],\n",
              "       [0.46891385],\n",
              "       [0.45627937],\n",
              "       [0.49378172],\n",
              "       [0.4477097 ],\n",
              "       [0.4461953 ],\n",
              "       [0.463108  ],\n",
              "       [0.41330272],\n",
              "       [0.41154996],\n",
              "       [0.44991273],\n",
              "       [0.46262997],\n",
              "       [0.47633207],\n",
              "       [0.3981961 ],\n",
              "       [0.47695425],\n",
              "       [0.44404885],\n",
              "       [0.47313362],\n",
              "       [0.505169  ],\n",
              "       [0.42694902],\n",
              "       [0.50822616],\n",
              "       [0.50433177],\n",
              "       [0.48023325],\n",
              "       [0.49503702],\n",
              "       [0.47408032],\n",
              "       [0.4562799 ],\n",
              "       [0.48489583],\n",
              "       [0.4553424 ],\n",
              "       [0.4436735 ],\n",
              "       [0.44164348],\n",
              "       [0.46817422],\n",
              "       [0.46620068],\n",
              "       [0.43073404],\n",
              "       [0.49816185],\n",
              "       [0.48813838],\n",
              "       [0.5091421 ],\n",
              "       [0.528683  ],\n",
              "       [0.48468128],\n",
              "       [0.46401927],\n",
              "       [0.4845292 ],\n",
              "       [0.47595477],\n",
              "       [0.50186366],\n",
              "       [0.49653554],\n",
              "       [0.48732823],\n",
              "       [0.4646142 ],\n",
              "       [0.46917787],\n",
              "       [0.457601  ],\n",
              "       [0.48713535],\n",
              "       [0.45756772],\n",
              "       [0.4418791 ],\n",
              "       [0.44122007],\n",
              "       [0.47517115],\n",
              "       [0.4787529 ],\n",
              "       [0.49211732],\n",
              "       [0.42836243],\n",
              "       [0.44950983],\n",
              "       [0.47978172],\n",
              "       [0.4792688 ],\n",
              "       [0.47484902],\n",
              "       [0.43215537],\n",
              "       [0.4491043 ],\n",
              "       [0.46211022],\n",
              "       [0.4367572 ],\n",
              "       [0.45273152],\n",
              "       [0.4674652 ],\n",
              "       [0.4151019 ],\n",
              "       [0.44742614],\n",
              "       [0.42243147],\n",
              "       [0.49633807],\n",
              "       [0.4677034 ],\n",
              "       [0.45268255],\n",
              "       [0.49461463],\n",
              "       [0.44439593],\n",
              "       [0.49498135],\n",
              "       [0.46698993],\n",
              "       [0.4521623 ],\n",
              "       [0.4620621 ],\n",
              "       [0.4937976 ],\n",
              "       [0.4148729 ],\n",
              "       [0.45029607],\n",
              "       [0.4425791 ],\n",
              "       [0.4646111 ],\n",
              "       [0.4467281 ],\n",
              "       [0.4637235 ],\n",
              "       [0.46657142],\n",
              "       [0.4464511 ],\n",
              "       [0.47619316],\n",
              "       [0.47708756],\n",
              "       [0.49111915],\n",
              "       [0.49172455],\n",
              "       [0.4913707 ],\n",
              "       [0.47445065],\n",
              "       [0.4591403 ],\n",
              "       [0.49523264],\n",
              "       [0.48014942],\n",
              "       [0.47227055],\n",
              "       [0.48679608],\n",
              "       [0.4487427 ],\n",
              "       [0.46350035],\n",
              "       [0.43863875],\n",
              "       [0.446297  ],\n",
              "       [0.4786869 ],\n",
              "       [0.48005694],\n",
              "       [0.45217398],\n",
              "       [0.45894325],\n",
              "       [0.46521956],\n",
              "       [0.50703233],\n",
              "       [0.44194233],\n",
              "       [0.42576227],\n",
              "       [0.47399357],\n",
              "       [0.47578886],\n",
              "       [0.5062883 ],\n",
              "       [0.48750407],\n",
              "       [0.50763226],\n",
              "       [0.5164177 ],\n",
              "       [0.46456617],\n",
              "       [0.5079418 ],\n",
              "       [0.52974033],\n",
              "       [0.49008876],\n",
              "       [0.5024306 ],\n",
              "       [0.46575674],\n",
              "       [0.4766922 ],\n",
              "       [0.5092448 ],\n",
              "       [0.5261855 ],\n",
              "       [0.5223291 ],\n",
              "       [0.46813148],\n",
              "       [0.5153701 ],\n",
              "       [0.50076187],\n",
              "       [0.49760884],\n",
              "       [0.51695037],\n",
              "       [0.5216666 ],\n",
              "       [0.5571528 ],\n",
              "       [0.5421348 ],\n",
              "       [0.5046971 ],\n",
              "       [0.55404305],\n",
              "       [0.5171986 ],\n",
              "       [0.5153663 ],\n",
              "       [0.5178579 ],\n",
              "       [0.5084825 ],\n",
              "       [0.505268  ],\n",
              "       [0.48286307],\n",
              "       [0.516789  ],\n",
              "       [0.50934774],\n",
              "       [0.46754366],\n",
              "       [0.52510935],\n",
              "       [0.5050749 ],\n",
              "       [0.47605884],\n",
              "       [0.5031252 ],\n",
              "       [0.47689414],\n",
              "       [0.5127251 ],\n",
              "       [0.4866935 ],\n",
              "       [0.51080835],\n",
              "       [0.54568744],\n",
              "       [0.5096881 ],\n",
              "       [0.5075024 ],\n",
              "       [0.461792  ],\n",
              "       [0.4446457 ],\n",
              "       [0.50708044],\n",
              "       [0.45782745],\n",
              "       [0.44620913],\n",
              "       [0.48926505],\n",
              "       [0.40801   ],\n",
              "       [0.5124    ],\n",
              "       [0.44319943],\n",
              "       [0.3961781 ],\n",
              "       [0.4561996 ],\n",
              "       [0.4487093 ],\n",
              "       [0.46130815],\n",
              "       [0.48380244],\n",
              "       [0.49442375],\n",
              "       [0.48507148],\n",
              "       [0.48920056],\n",
              "       [0.46895182],\n",
              "       [0.49322826],\n",
              "       [0.45691946],\n",
              "       [0.49037927],\n",
              "       [0.3765013 ],\n",
              "       [0.4563841 ],\n",
              "       [0.42873836],\n",
              "       [0.4767853 ],\n",
              "       [0.4502691 ],\n",
              "       [0.45796457],\n",
              "       [0.46802968],\n",
              "       [0.4456303 ],\n",
              "       [0.47901136],\n",
              "       [0.44233507],\n",
              "       [0.4410889 ],\n",
              "       [0.49765837],\n",
              "       [0.44149864],\n",
              "       [0.4473293 ],\n",
              "       [0.44873106],\n",
              "       [0.47274956],\n",
              "       [0.4427969 ],\n",
              "       [0.3979017 ],\n",
              "       [0.4233055 ],\n",
              "       [0.47297978],\n",
              "       [0.38262057],\n",
              "       [0.51152533],\n",
              "       [0.44173393],\n",
              "       [0.4688802 ],\n",
              "       [0.52384895],\n",
              "       [0.457754  ],\n",
              "       [0.47484416],\n",
              "       [0.45033562],\n",
              "       [0.40627718],\n",
              "       [0.47056317]], dtype=float32)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fEfMMBwvPNa",
        "outputId": "1eb4a9ba-4268-4dcc-90f4-aca66287174d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(468, 1)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJhQJWfNvPNa",
        "outputId": "2bc7ba87-53a0-4a13-e366-0d9067516970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(468,)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test[:,1][win_len:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_RkBnzZgvPNb",
        "outputId": "a4d5c883-f7e7-4c2f-b3d6-0a1c618ab119"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Pred  Actual\n",
              "0   0.0     1.0\n",
              "1   1.0     1.0\n",
              "2   0.0     0.0\n",
              "3   1.0     1.0\n",
              "4   1.0     0.0"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pred = pd.concat([pd.DataFrame(np.round(predictions)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NVr4vWEhvPNb"
      },
      "outputs": [],
      "source": [
        "#function to print accuracy and MCC of the model\n",
        "def evaluation(df : pd.DataFrame):\n",
        "    conf = pd.crosstab(df[\"Actual\"], df[\"Pred\"])\n",
        "    fig = plt.figure(figsize = (6,4))\n",
        "    sn.heatmap(conf, annot = True, cmap = \"Blues\", fmt = \"g\")\n",
        "    plt.title(\"Confussion Matrix\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    TP = conf[1][1]\n",
        "    FN = conf[0][1]\n",
        "    FP = conf[1][0]\n",
        "    TN = conf[0][0]\n",
        "    Acc = (TP+TN)/(TP+TN+FN+FP)\n",
        "    Mcc = (TP*TN - FP*FN) / np.sqrt( (TP + FP)*(TP + FN)*(TN + FP)*(TN + FN) )\n",
        "    print(\"Accuracy =\",Acc)\n",
        "    print(\"MCC =\",Mcc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "4QNaOzY7vPNc",
        "outputId": "0f8ff4c9-4698-44a4-af07-a5074d3bf0cb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.5213675213675214\n",
            "MCC = 0.07524606050562324\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UvOYGxuzAag"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUgTlzy10QFE"
      },
      "source": [
        "<center><h3>Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "71grCVCf0Spb"
      },
      "outputs": [],
      "source": [
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model2 = build_model(input_shape, head_size=512, num_heads=6, ff_dim=4, num_transformer_blocks=4, mlp_units=[256], \n",
        "                    mlp_dropout=0.4,dropout=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m4Q18_i06QE",
        "outputId": "2392167c-0c84-422d-ea82-de71badf0afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 30, 12)      24          ['input_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 30, 12)      156684      ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 30, 12)      0           ['dropout_13[0][0]',             \n",
            " mbda)                                                            'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 30, 4)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 30, 12)       60          ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 30, 12)      0           ['conv1d_9[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 30, 12)      156684      ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 30, 12)      0           ['dropout_15[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 30, 4)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 30, 12)       60          ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 30, 12)      0           ['conv1d_11[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 30, 12)      156684      ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 30, 12)      0           ['dropout_17[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 30, 4)        0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 30, 12)       60          ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 30, 12)      0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 30, 12)      156684      ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 30, 12)      0           ['dropout_19[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 30, 4)        0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 30, 12)       60          ['dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 30, 12)      0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 30)          0           ['tf.__operators__.add_15[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          7936        ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 256)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            257         ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 635,569\n",
            "Trainable params: 635,569\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "jpr3n4Yw09Wf"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model2.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model2.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzsgBOw51HrQ",
        "outputId": "65be77cf-4dcd-4324-a902-2d13aa0743de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45/46 [============================>.] - ETA: 0s - loss: 2.4829 - accuracy: 0.5188\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51496, saving model to clas_logs\\model2.hdf5\n",
            "46/46 [==============================] - 6s 45ms/step - loss: 2.4767 - accuracy: 0.5202 - val_loss: 0.9136 - val_accuracy: 0.5150\n",
            "Epoch 2/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 4.5413 - accuracy: 0.4709\n",
            "Epoch 2: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 4.4825 - accuracy: 0.4723 - val_loss: 1.0786 - val_accuracy: 0.4850\n",
            "Epoch 3/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 3.5009 - accuracy: 0.4701\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 3.4617 - accuracy: 0.4709 - val_loss: 1.0380 - val_accuracy: 0.4850\n",
            "Epoch 4/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.4522 - accuracy: 0.4921\n",
            "Epoch 4: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 2.4522 - accuracy: 0.4921 - val_loss: 6.8155 - val_accuracy: 0.5150\n",
            "Epoch 5/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 4.0865 - accuracy: 0.5284\n",
            "Epoch 5: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 4.0865 - accuracy: 0.5284 - val_loss: 0.8654 - val_accuracy: 0.4850\n",
            "Epoch 6/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 4.9816 - accuracy: 0.4976\n",
            "Epoch 6: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 4.9816 - accuracy: 0.4976 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 7/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 7: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 8/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 8: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 9/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 9: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 10/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 10: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 11/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 11: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 12/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 12: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 13/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 13: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 14/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 14: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 15/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 15: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 16/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 16: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 17/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 17: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 18/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 18: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 19/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 19: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 20/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 20: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 21/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 21: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 22/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 22: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 23/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 23: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 24/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9858 - accuracy: 0.5414\n",
            "Epoch 24: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9858 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 25/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.9914 - accuracy: 0.5376\n",
            "Epoch 25: val_accuracy improved from 0.51496 to 0.51923, saving model to clas_logs\\model2.hdf5\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 5.8888 - accuracy: 0.5387 - val_loss: 0.9628 - val_accuracy: 0.5192\n",
            "Epoch 26/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.9506 - accuracy: 0.5348\n",
            "Epoch 26: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 5.8444 - accuracy: 0.5380 - val_loss: 7.3094 - val_accuracy: 0.5150\n",
            "Epoch 27/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.8786 - accuracy: 0.5256\n",
            "Epoch 27: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 5.8337 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 28/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0511 - accuracy: 0.5376\n",
            "Epoch 28: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9727 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9844 - accuracy: 0.5414\n",
            "Epoch 29: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9844 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9062 - accuracy: 0.5387\n",
            "Epoch 30: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9062 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0207 - accuracy: 0.5299\n",
            "Epoch 31: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9928 - accuracy: 0.5318 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 32: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0742 - accuracy: 0.5355\n",
            "Epoch 33: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9950 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 34: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0727 - accuracy: 0.5362\n",
            "Epoch 35: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9936 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9829 - accuracy: 0.5421\n",
            "Epoch 36: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9932 - accuracy: 0.5414\n",
            "Epoch 37: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0010 - accuracy: 0.5403\n",
            "Epoch 38: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9735 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 39: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0618 - accuracy: 0.5369\n",
            "Epoch 40: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9831 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 41: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0109 - accuracy: 0.5396\n",
            "Epoch 42: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9832 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0728 - accuracy: 0.5355\n",
            "Epoch 43: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9939 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 44: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 45: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0724 - accuracy: 0.5362\n",
            "Epoch 46: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9933 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 47/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9934 - accuracy: 0.5414\n",
            "Epoch 47: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9934 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9935 - accuracy: 0.5414\n",
            "Epoch 48: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9935 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0622 - accuracy: 0.5362\n",
            "Epoch 49: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9834 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0832 - accuracy: 0.5355\n",
            "Epoch 50: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 51/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 51: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 52/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 52: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0629 - accuracy: 0.5369\n",
            "Epoch 53: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9841 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0199 - accuracy: 0.5355\n",
            "Epoch 54: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9410 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.6736 - accuracy: 0.5319\n",
            "Epoch 55: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.6508 - accuracy: 0.5339 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.3739 - accuracy: 0.5359\n",
            "Epoch 56: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.3739 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9604 - accuracy: 0.5354\n",
            "Epoch 57: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9334 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0588 - accuracy: 0.5334\n",
            "Epoch 58: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9802 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9256 - accuracy: 0.5448\n",
            "Epoch 59: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9256 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0065 - accuracy: 0.5368\n",
            "Epoch 60: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9788 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9595 - accuracy: 0.5382\n",
            "Epoch 61: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9325 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.5118 - accuracy: 0.5410\n",
            "Epoch 62: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.4913 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.8937 - accuracy: 0.5284\n",
            "Epoch 63: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.8211 - accuracy: 0.5339 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.4110 - accuracy: 0.5028\n",
            "Epoch 64: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.3099 - accuracy: 0.5099 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1106 - accuracy: 0.5071\n",
            "Epoch 65: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.0996 - accuracy: 0.5072 - val_loss: 6.3712 - val_accuracy: 0.5128\n",
            "Epoch 66/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9952 - accuracy: 0.5414\n",
            "Epoch 66: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 6.9952 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.1198 - accuracy: 0.5319\n",
            "Epoch 67: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0905 - accuracy: 0.5339 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0577 - accuracy: 0.5368\n",
            "Epoch 68: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0293 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1075 - accuracy: 0.5334\n",
            "Epoch 69: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.0271 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0337 - accuracy: 0.5382\n",
            "Epoch 70: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0057 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1059 - accuracy: 0.5348\n",
            "Epoch 71: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0256 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0658 - accuracy: 0.5368\n",
            "Epoch 72: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0373 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0858 - accuracy: 0.5354\n",
            "Epoch 73: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0570 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.1043 - accuracy: 0.5333\n",
            "Epoch 74: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0753 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9202 - accuracy: 0.5437\n",
            "Epoch 75: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.8938 - accuracy: 0.5455 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0798 - accuracy: 0.5334\n",
            "Epoch 76: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0004 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.1012 - accuracy: 0.5346\n",
            "Epoch 77: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.1012 - accuracy: 0.5346 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0370 - accuracy: 0.5387\n",
            "Epoch 78: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0370 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0652 - accuracy: 0.5375\n",
            "Epoch 79: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0367 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1285 - accuracy: 0.5334\n",
            "Epoch 80: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.0473 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.2138 - accuracy: 0.5256\n",
            "Epoch 81: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.1296 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0820 - accuracy: 0.5327\n",
            "Epoch 82: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 7.0025 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0758 - accuracy: 0.5368\n",
            "Epoch 83: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0472 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0484 - accuracy: 0.5380\n",
            "Epoch 84: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0484 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0464 - accuracy: 0.5368\n",
            "Epoch 85: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0181 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1629 - accuracy: 0.5298\n",
            "Epoch 86: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0805 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1418 - accuracy: 0.5320\n",
            "Epoch 87: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0602 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0556 - accuracy: 0.5375\n",
            "Epoch 88: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0273 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0553 - accuracy: 0.5375\n",
            "Epoch 89: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0269 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0569 - accuracy: 0.5375\n",
            "Epoch 90: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 7.0286 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0356 - accuracy: 0.5389\n",
            "Epoch 91: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0076 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0249 - accuracy: 0.5396\n",
            "Epoch 92: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9970 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0569 - accuracy: 0.5375\n",
            "Epoch 93: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0286 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0462 - accuracy: 0.5382\n",
            "Epoch 94: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 7.0180 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0358 - accuracy: 0.5389\n",
            "Epoch 95: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0078 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1273 - accuracy: 0.5334\n",
            "Epoch 96: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0462 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0750 - accuracy: 0.5368\n",
            "Epoch 97: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0463 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0642 - accuracy: 0.5375\n",
            "Epoch 98: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0358 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0644 - accuracy: 0.5375\n",
            "Epoch 99: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0359 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0880 - accuracy: 0.5361\n",
            "Epoch 100: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0592 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.1564 - accuracy: 0.5318\n",
            "Epoch 101: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.1564 - accuracy: 0.5318 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.2464 - accuracy: 0.5243\n",
            "Epoch 102: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.2153 - accuracy: 0.5264 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 103/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0645 - accuracy: 0.5368\n",
            "Epoch 103: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0360 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0420 - accuracy: 0.5369\n",
            "Epoch 104: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9640 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 105: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 106/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 106: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 107: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 108: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 109: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 110: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 111: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 113: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 114: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 116: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 117: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 118: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 119: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 120: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 121: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 122: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 123: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 124: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 125: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 126: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 127: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 128: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 129: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 130: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 131: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 133: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 134: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 135: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 136: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 137: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 138: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 139: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 140: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 142: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 143: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 144: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 145: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 146: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 147: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 148: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 150: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 151/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 151: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 152/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 152: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 153/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 153: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 154/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 154: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 155/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 155: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 156/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 156: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 157/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 157: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 158/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 158: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 159/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 159: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 160/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 160: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 161/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 161: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 162/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 162: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 163/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 163: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 164/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 164: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 165/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 165: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 166/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 166: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 167/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 167: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 168/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 168: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 169/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 169: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 170/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 170: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 171/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 171: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 172/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 172: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 173/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 173: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 174/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 174: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 175/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 175: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 176/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 176: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 177/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 177: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 178/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 178: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 179/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 179: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 180/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 180: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 181/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 181: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 182/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 182: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 183/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 183: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 184/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 184: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 185/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 185: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 186/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 186: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 187/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 187: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 188/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 188: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 189/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 189: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 190/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 190: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 191/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 191: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 192/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 192: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 193/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 193: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 194/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 194: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 195/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 195: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 196/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 196: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 197/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 197: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 198/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 198: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 199/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 199: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 200/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 200: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 201/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 201: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 202/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 202: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 203/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 203: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 204/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 204: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 205/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 205: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 206/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 206: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 207/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 207: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 208/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 208: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 209/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 209: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 210/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 210: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 211/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 211: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 212/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 212: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 213/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 213: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 214/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 214: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 215/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 215: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 216/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 216: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 217/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 217: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 218/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 218: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 219/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 219: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 220/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 220: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 221/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 221: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 222/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 222: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 223/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 223: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 224/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 224: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 225/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 225: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 226/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 226: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 227/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 227: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 228/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 228: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 229/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 229: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 230/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 230: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 231/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 231: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 232/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 232: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 233/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 233: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 234/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 234: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 235/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 235: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 236/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 236: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 237/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 237: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 238/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 238: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 239/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 239: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 240/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 240: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 241/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 241: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 242/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 242: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 243/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 243: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 244/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 244: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 245/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 245: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 246/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 246: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 247/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 247: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 248/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 248: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 249/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 249: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 250/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 250: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 251/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 251: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 252/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 252: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 253/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 253: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 254/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 254: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 255/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 255: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 256/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 256: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 257/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 257: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 258/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 258: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 259/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 259: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 260/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 260: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 261/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 261: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 262/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 262: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 263/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 263: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 264/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 264: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 265/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 265: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 266/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 266: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 267/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 267: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 268/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 268: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 269/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 269: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 270/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 270: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 271/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 271: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 272/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 272: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 273/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 273: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 274/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 274: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 275/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 275: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 276/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 276: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 277/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 277: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 278/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 278: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 279/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 279: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 280/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 280: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 281/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 281: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 282/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 282: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 283/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 283: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 284/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 284: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 285/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 285: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 286/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 286: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 287/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 287: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 288/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 288: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 289/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 289: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 290/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 290: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 291/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 291: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 292/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 292: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 293/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 293: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 294/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 294: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 295/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 295: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 296/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 296: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 297/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 297: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 298/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 298: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 299/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 299: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 300/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 300: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 301/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 301: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 302/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 302: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 303/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 303: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 304/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 304: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 305/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 305: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 306/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 306: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 307/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 307: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 308/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 308: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 309/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 309: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 310/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 310: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 311/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 311: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 312/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 312: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 313/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 313: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 314/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 314: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 315/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 315: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 316/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 316: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 317/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 317: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 318/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 318: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 319/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 319: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 320/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 320: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 321/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 321: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 322/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 322: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 323/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 323: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 324/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 324: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 325/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 325: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 326/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 326: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 327/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 327: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 328/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 328: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 329/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 329: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 330/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 330: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 331/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 331: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 332/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 332: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 333/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 333: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 334/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 334: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 335/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 335: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 336/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 336: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 337/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 337: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 338/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 338: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 339/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 339: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 340/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 340: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 341/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 341: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 342/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 342: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 343/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 343: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 344/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 344: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 345/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 345: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 346/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 346: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 347/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 347: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 348/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 348: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 349/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 349: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 350/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 350: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 351/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 351: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 352/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 352: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 353/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 353: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 354/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 354: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 355/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 355: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 356/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 356: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 357/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 357: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 358/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 358: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 359/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 359: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 360/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 360: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 361/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 361: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 362/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 362: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 363/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 363: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 364/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 364: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 365/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 365: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 366/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 366: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 367/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 367: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 368/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 368: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 369/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 369: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 370/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 370: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 371/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 371: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 372/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 372: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 373/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 373: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 374/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 374: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 375/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 375: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 376/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 376: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 377/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 377: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 378/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 378: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 379/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 379: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 380/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 380: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 381/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 381: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 382/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 382: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 383/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 383: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 384/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 384: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 385/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 385: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 386/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 386: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 387/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 387: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 388/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 388: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 389/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 389: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 390/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 390: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 391/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 391: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 392/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 392: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 393/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 393: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 394/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 394: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 395/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 395: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 396/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 396: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 397/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 397: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 398/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 398: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 399/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 399: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 400/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 400: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 401/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 401: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 402/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 402: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 403/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 403: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 404/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 404: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 405/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 405: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 406/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 406: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 407/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 407: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 408/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 408: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 409/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 409: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 410/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 410: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 411/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 411: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 412/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 412: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 413/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 413: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 414/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 414: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 415/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 415: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 416/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 416: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 417/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 417: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 418/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 418: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 419/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 419: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 420/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 420: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 421/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 421: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 422/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 422: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 423/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 423: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 424/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 424: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 425/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 425: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 426/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 426: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 427/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 427: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 428/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 428: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 429/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 429: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 430/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 430: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 431/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 431: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 432/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 432: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 433/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 433: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 434/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 434: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 435/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 435: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 436/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 436: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 437/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 437: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 438/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 438: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 439/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 439: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 440/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 440: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 441/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 441: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 442/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 442: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 443/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 443: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 444/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 444: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 445/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 445: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 446/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 446: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 447/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 447: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 448/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 448: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 449/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 449: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 450/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 450: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 451/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 451: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 452/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 452: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 453/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 453: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 454/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 454: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 455/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 455: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 456/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 456: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 457/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 457: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 458/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 458: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 459/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 459: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 460/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 460: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 461/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 461: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 462/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 462: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 463/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 463: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 464/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 464: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 465/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 465: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 466/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 466: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 467/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 467: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 468/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 468: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 469/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 469: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 470/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 470: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 471/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 471: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 472/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 472: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 473/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 473: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 474/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 474: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 475/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 475: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 476/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 476: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 477/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 477: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 478/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 478: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 479/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 479: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 480/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 480: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 481/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 481: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 482/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 482: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 483/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 483: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 484/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 484: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 485/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 485: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 486/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 486: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 487/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 487: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 488/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 488: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 489/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 489: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 490/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 490: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 491/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 491: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 492/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 492: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 493/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 493: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 494/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 494: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 495/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 495: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 496/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 496: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 497/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 497: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 498/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 498: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 499/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 499: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 500/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 500: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "# history = model1.fit()\n",
        "history = model2.fit_generator(train_generator, epochs=500, validation_data=test_generator, shuffle=False, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "CHoi2w-p1LAb",
        "outputId": "13490629-f549-455a-88eb-344b792aa013"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plotHist(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "-wWalIzW1QZ5",
        "outputId": "e0b351b7-eeeb-4fdc-b149-dff8b0b9cc6e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plotHist(history, \"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "E7uOWF8J1Sfj"
      },
      "outputs": [],
      "source": [
        "model2 = tf.keras.models.load_model(\"/content/clas_logs\\model2.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h42jMki1WTR",
        "outputId": "05ac955c-f58b-4a16-dec4-dbbec767a1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1s 12ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model2.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzrzTL3u1Z4S",
        "outputId": "3e3a5073-c684-4bb9-be21-921d94af478e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.5604003 ],\n",
              "       [0.5840023 ],\n",
              "       [0.56813335],\n",
              "       [0.56071943],\n",
              "       [0.47787932],\n",
              "       [0.5365058 ],\n",
              "       [0.5902235 ],\n",
              "       [0.5785546 ],\n",
              "       [0.5498153 ],\n",
              "       [0.6415742 ],\n",
              "       [0.53546226],\n",
              "       [0.6179971 ],\n",
              "       [0.61814517],\n",
              "       [0.5368658 ],\n",
              "       [0.7045175 ],\n",
              "       [0.67061776],\n",
              "       [0.7198985 ],\n",
              "       [0.734953  ],\n",
              "       [0.8333355 ],\n",
              "       [0.7881227 ],\n",
              "       [0.6757453 ],\n",
              "       [0.71304834],\n",
              "       [0.7676613 ],\n",
              "       [0.6678465 ],\n",
              "       [0.7550792 ],\n",
              "       [0.68683356],\n",
              "       [0.64809024],\n",
              "       [0.6061692 ],\n",
              "       [0.61877835],\n",
              "       [0.5434183 ],\n",
              "       [0.5586975 ],\n",
              "       [0.57402533],\n",
              "       [0.57407725],\n",
              "       [0.61538106],\n",
              "       [0.5204029 ],\n",
              "       [0.5324075 ],\n",
              "       [0.45486426],\n",
              "       [0.4102744 ],\n",
              "       [0.41932562],\n",
              "       [0.53276086],\n",
              "       [0.5976053 ],\n",
              "       [0.6085633 ],\n",
              "       [0.65762895],\n",
              "       [0.69663244],\n",
              "       [0.6653266 ],\n",
              "       [0.6685642 ],\n",
              "       [0.5787448 ],\n",
              "       [0.63836473],\n",
              "       [0.7167022 ],\n",
              "       [0.67313   ],\n",
              "       [0.66699475],\n",
              "       [0.6182877 ],\n",
              "       [0.70241576],\n",
              "       [0.6396772 ],\n",
              "       [0.629664  ],\n",
              "       [0.77225894],\n",
              "       [0.75358605],\n",
              "       [0.7630865 ],\n",
              "       [0.69050825],\n",
              "       [0.6469148 ],\n",
              "       [0.71687835],\n",
              "       [0.6573895 ],\n",
              "       [0.63488346],\n",
              "       [0.7001088 ],\n",
              "       [0.84111995],\n",
              "       [0.79513466],\n",
              "       [0.80734843],\n",
              "       [0.9122022 ],\n",
              "       [0.8900409 ],\n",
              "       [0.9056685 ],\n",
              "       [0.86248285],\n",
              "       [0.84294957],\n",
              "       [0.77707374],\n",
              "       [0.7277216 ],\n",
              "       [0.661259  ],\n",
              "       [0.5304237 ],\n",
              "       [0.6170886 ],\n",
              "       [0.56699055],\n",
              "       [0.4794568 ],\n",
              "       [0.52254623],\n",
              "       [0.5833598 ],\n",
              "       [0.54329854],\n",
              "       [0.46722573],\n",
              "       [0.47254598],\n",
              "       [0.5420855 ],\n",
              "       [0.5138467 ],\n",
              "       [0.6360152 ],\n",
              "       [0.5067848 ],\n",
              "       [0.5628863 ],\n",
              "       [0.51820886],\n",
              "       [0.46633583],\n",
              "       [0.49326763],\n",
              "       [0.4400077 ],\n",
              "       [0.4303951 ],\n",
              "       [0.5015593 ],\n",
              "       [0.42475763],\n",
              "       [0.35429356],\n",
              "       [0.27736908],\n",
              "       [0.34136283],\n",
              "       [0.25297615],\n",
              "       [0.25717774],\n",
              "       [0.29094788],\n",
              "       [0.3527296 ],\n",
              "       [0.3819831 ],\n",
              "       [0.4044187 ],\n",
              "       [0.39199194],\n",
              "       [0.4135442 ],\n",
              "       [0.41382256],\n",
              "       [0.41292658],\n",
              "       [0.3987528 ],\n",
              "       [0.43392938],\n",
              "       [0.5133337 ],\n",
              "       [0.5080798 ],\n",
              "       [0.41836187],\n",
              "       [0.3630798 ],\n",
              "       [0.329293  ],\n",
              "       [0.31396264],\n",
              "       [0.2999027 ],\n",
              "       [0.38334602],\n",
              "       [0.42556456],\n",
              "       [0.41876698],\n",
              "       [0.46181402],\n",
              "       [0.43638113],\n",
              "       [0.41107416],\n",
              "       [0.36680612],\n",
              "       [0.35669598],\n",
              "       [0.41794506],\n",
              "       [0.5370115 ],\n",
              "       [0.4950547 ],\n",
              "       [0.6149491 ],\n",
              "       [0.5846561 ],\n",
              "       [0.39445853],\n",
              "       [0.35637438],\n",
              "       [0.38957432],\n",
              "       [0.45578873],\n",
              "       [0.39103314],\n",
              "       [0.3886776 ],\n",
              "       [0.48772216],\n",
              "       [0.42125005],\n",
              "       [0.45187864],\n",
              "       [0.4411867 ],\n",
              "       [0.39181593],\n",
              "       [0.39899075],\n",
              "       [0.34405133],\n",
              "       [0.34295768],\n",
              "       [0.40864483],\n",
              "       [0.30304328],\n",
              "       [0.38652608],\n",
              "       [0.32998654],\n",
              "       [0.33470035],\n",
              "       [0.32577953],\n",
              "       [0.35758087],\n",
              "       [0.3575972 ],\n",
              "       [0.37388197],\n",
              "       [0.3317431 ],\n",
              "       [0.29453272],\n",
              "       [0.28865483],\n",
              "       [0.34986424],\n",
              "       [0.34819576],\n",
              "       [0.36547935],\n",
              "       [0.3673348 ],\n",
              "       [0.39912802],\n",
              "       [0.3169572 ],\n",
              "       [0.33965898],\n",
              "       [0.2819489 ],\n",
              "       [0.386377  ],\n",
              "       [0.39335865],\n",
              "       [0.3834749 ],\n",
              "       [0.3579772 ],\n",
              "       [0.37919554],\n",
              "       [0.42412335],\n",
              "       [0.32398427],\n",
              "       [0.28287777],\n",
              "       [0.4398693 ],\n",
              "       [0.39574662],\n",
              "       [0.3948544 ],\n",
              "       [0.42152855],\n",
              "       [0.5663619 ],\n",
              "       [0.41108248],\n",
              "       [0.34791481],\n",
              "       [0.34441894],\n",
              "       [0.47938138],\n",
              "       [0.43173364],\n",
              "       [0.40173438],\n",
              "       [0.42618117],\n",
              "       [0.4766358 ],\n",
              "       [0.4598785 ],\n",
              "       [0.42244866],\n",
              "       [0.3447721 ],\n",
              "       [0.35589707],\n",
              "       [0.34477916],\n",
              "       [0.389438  ],\n",
              "       [0.41218922],\n",
              "       [0.4092433 ],\n",
              "       [0.4808823 ],\n",
              "       [0.37476727],\n",
              "       [0.31415194],\n",
              "       [0.3097265 ],\n",
              "       [0.35596195],\n",
              "       [0.27926257],\n",
              "       [0.3038874 ],\n",
              "       [0.2954623 ],\n",
              "       [0.29438606],\n",
              "       [0.3484207 ],\n",
              "       [0.37819332],\n",
              "       [0.40800866],\n",
              "       [0.36143687],\n",
              "       [0.34487337],\n",
              "       [0.502441  ],\n",
              "       [0.47129574],\n",
              "       [0.49517056],\n",
              "       [0.47687694],\n",
              "       [0.42746648],\n",
              "       [0.4673361 ],\n",
              "       [0.45239758],\n",
              "       [0.449663  ],\n",
              "       [0.50005543],\n",
              "       [0.5055242 ],\n",
              "       [0.50594395],\n",
              "       [0.4497464 ],\n",
              "       [0.48571986],\n",
              "       [0.6615476 ],\n",
              "       [0.6166329 ],\n",
              "       [0.5822589 ],\n",
              "       [0.666543  ],\n",
              "       [0.7302948 ],\n",
              "       [0.64230776],\n",
              "       [0.68987787],\n",
              "       [0.6968923 ],\n",
              "       [0.7464623 ],\n",
              "       [0.72126436],\n",
              "       [0.64297855],\n",
              "       [0.6475914 ],\n",
              "       [0.6639966 ],\n",
              "       [0.5585246 ],\n",
              "       [0.5761175 ],\n",
              "       [0.5744321 ],\n",
              "       [0.5078565 ],\n",
              "       [0.476436  ],\n",
              "       [0.40930992],\n",
              "       [0.42882493],\n",
              "       [0.4426181 ],\n",
              "       [0.47151497],\n",
              "       [0.45409754],\n",
              "       [0.4288047 ],\n",
              "       [0.4450131 ],\n",
              "       [0.5421216 ],\n",
              "       [0.4375645 ],\n",
              "       [0.49855748],\n",
              "       [0.41936418],\n",
              "       [0.44533435],\n",
              "       [0.3756445 ],\n",
              "       [0.2838136 ],\n",
              "       [0.34827206],\n",
              "       [0.41675934],\n",
              "       [0.364996  ],\n",
              "       [0.2615537 ],\n",
              "       [0.3125242 ],\n",
              "       [0.40550098],\n",
              "       [0.3815383 ],\n",
              "       [0.40085468],\n",
              "       [0.48645526],\n",
              "       [0.5486174 ],\n",
              "       [0.55344266],\n",
              "       [0.5512047 ],\n",
              "       [0.57650846],\n",
              "       [0.6613045 ],\n",
              "       [0.7633025 ],\n",
              "       [0.7871364 ],\n",
              "       [0.77516794],\n",
              "       [0.8586177 ],\n",
              "       [0.8991282 ],\n",
              "       [0.8268637 ],\n",
              "       [0.7812682 ],\n",
              "       [0.84144664],\n",
              "       [0.86237425],\n",
              "       [0.80009705],\n",
              "       [0.8876297 ],\n",
              "       [0.9378899 ],\n",
              "       [0.99432087],\n",
              "       [1.1331668 ],\n",
              "       [1.1282142 ],\n",
              "       [1.1278689 ],\n",
              "       [1.1647288 ],\n",
              "       [1.1793073 ],\n",
              "       [1.2437073 ],\n",
              "       [1.2354023 ],\n",
              "       [1.2272557 ],\n",
              "       [1.189113  ],\n",
              "       [1.1482365 ],\n",
              "       [1.2183139 ],\n",
              "       [1.123038  ],\n",
              "       [1.1889875 ],\n",
              "       [1.1521142 ],\n",
              "       [1.1028322 ],\n",
              "       [1.0031972 ],\n",
              "       [0.875742  ],\n",
              "       [0.8737371 ],\n",
              "       [0.96188855],\n",
              "       [0.8724465 ],\n",
              "       [0.82121485],\n",
              "       [0.933592  ],\n",
              "       [0.88182956],\n",
              "       [0.854996  ],\n",
              "       [0.8709644 ],\n",
              "       [0.8616231 ],\n",
              "       [0.7909596 ],\n",
              "       [0.75338185],\n",
              "       [0.6647425 ],\n",
              "       [0.61965406],\n",
              "       [0.70268446],\n",
              "       [0.6378503 ],\n",
              "       [0.56236327],\n",
              "       [0.43866032],\n",
              "       [0.5355647 ],\n",
              "       [0.50457436],\n",
              "       [0.49614477],\n",
              "       [0.5050271 ],\n",
              "       [0.46874622],\n",
              "       [0.38863677],\n",
              "       [0.43004486],\n",
              "       [0.41094127],\n",
              "       [0.44526878],\n",
              "       [0.36038736],\n",
              "       [0.340027  ],\n",
              "       [0.36742017],\n",
              "       [0.3867241 ],\n",
              "       [0.36808303],\n",
              "       [0.44615808],\n",
              "       [0.3398169 ],\n",
              "       [0.38356653],\n",
              "       [0.39415303],\n",
              "       [0.39930275],\n",
              "       [0.43038195],\n",
              "       [0.40976146],\n",
              "       [0.39209166],\n",
              "       [0.41852346],\n",
              "       [0.4573968 ],\n",
              "       [0.47252652],\n",
              "       [0.49389446],\n",
              "       [0.4450539 ],\n",
              "       [0.4447382 ],\n",
              "       [0.34216493],\n",
              "       [0.39328262],\n",
              "       [0.40908757],\n",
              "       [0.40558466],\n",
              "       [0.34547532],\n",
              "       [0.38618165],\n",
              "       [0.4282438 ],\n",
              "       [0.42438158],\n",
              "       [0.39265576],\n",
              "       [0.51694095],\n",
              "       [0.5550259 ],\n",
              "       [0.53298503],\n",
              "       [0.50684077],\n",
              "       [0.60465467],\n",
              "       [0.68180716],\n",
              "       [0.68743664],\n",
              "       [0.7215529 ],\n",
              "       [0.6918906 ],\n",
              "       [0.810965  ],\n",
              "       [0.7884245 ],\n",
              "       [0.7635739 ],\n",
              "       [0.8115697 ],\n",
              "       [0.87675804],\n",
              "       [0.8277335 ],\n",
              "       [0.6986654 ],\n",
              "       [0.74548274],\n",
              "       [0.84088844],\n",
              "       [0.75739044],\n",
              "       [0.77214265],\n",
              "       [0.8336212 ],\n",
              "       [0.93854743],\n",
              "       [0.8700408 ],\n",
              "       [0.9228963 ],\n",
              "       [0.92165107],\n",
              "       [0.98585993],\n",
              "       [0.89403015],\n",
              "       [0.86403215],\n",
              "       [0.9499789 ],\n",
              "       [0.8848973 ],\n",
              "       [1.0223365 ],\n",
              "       [0.9611239 ],\n",
              "       [0.8962876 ],\n",
              "       [0.83796984],\n",
              "       [0.95363814],\n",
              "       [0.9398906 ],\n",
              "       [0.87161463],\n",
              "       [0.90959543],\n",
              "       [0.8654274 ],\n",
              "       [0.740273  ],\n",
              "       [0.80981433],\n",
              "       [0.86696607],\n",
              "       [0.8808825 ],\n",
              "       [0.84020454],\n",
              "       [0.90074575],\n",
              "       [0.82855386],\n",
              "       [0.8837278 ],\n",
              "       [0.89300364],\n",
              "       [1.0143851 ],\n",
              "       [0.95089304],\n",
              "       [0.9362199 ],\n",
              "       [0.87190884],\n",
              "       [0.9743344 ],\n",
              "       [0.9098886 ],\n",
              "       [0.8028731 ],\n",
              "       [0.8049751 ],\n",
              "       [0.77386016],\n",
              "       [0.7386116 ],\n",
              "       [0.8103538 ],\n",
              "       [0.77133304],\n",
              "       [0.87071943],\n",
              "       [0.78602654],\n",
              "       [0.7269364 ],\n",
              "       [0.69207126],\n",
              "       [0.6960273 ],\n",
              "       [0.84882236],\n",
              "       [0.729565  ],\n",
              "       [0.7688099 ],\n",
              "       [0.7990461 ],\n",
              "       [0.7884098 ],\n",
              "       [0.7248567 ],\n",
              "       [0.67123914],\n",
              "       [0.5573776 ],\n",
              "       [0.57674646],\n",
              "       [0.5697648 ],\n",
              "       [0.536105  ],\n",
              "       [0.537086  ],\n",
              "       [0.65484434],\n",
              "       [0.5235852 ],\n",
              "       [0.5199389 ],\n",
              "       [0.42169955],\n",
              "       [0.5550749 ],\n",
              "       [0.41953984],\n",
              "       [0.47506818],\n",
              "       [0.46878314],\n",
              "       [0.48468354],\n",
              "       [0.53868335],\n",
              "       [0.5667213 ],\n",
              "       [0.45741135],\n",
              "       [0.5270709 ],\n",
              "       [0.48525006],\n",
              "       [0.5088916 ],\n",
              "       [0.45602888],\n",
              "       [0.46327648],\n",
              "       [0.46336114],\n",
              "       [0.49615407],\n",
              "       [0.43183324],\n",
              "       [0.48281157],\n",
              "       [0.5063442 ],\n",
              "       [0.5258212 ],\n",
              "       [0.47389048],\n",
              "       [0.49677408],\n",
              "       [0.44774345],\n",
              "       [0.5004869 ],\n",
              "       [0.41100225],\n",
              "       [0.48743778],\n",
              "       [0.4300835 ],\n",
              "       [0.42186508],\n",
              "       [0.5144262 ],\n",
              "       [0.44689086],\n",
              "       [0.3846031 ],\n",
              "       [0.4391721 ],\n",
              "       [0.40559128],\n",
              "       [0.4008984 ],\n",
              "       [0.5098839 ],\n",
              "       [0.584966  ],\n",
              "       [0.66053385]], dtype=float32)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgz38c8Q1cfr",
        "outputId": "dac9430c-83ff-4990-f2f6-4dc17b0058aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(468, 1)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_SVkuy01eFz",
        "outputId": "ee5f48ac-735a-45d6-a187-d4634d69957a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(468,)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test[:,1][win_len:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dIVWSlyV1hXo",
        "outputId": "68d7744a-ebbe-4712-c9c3-611476ab9432"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-626fea44-ce20-4247-81b6-a7efd31eb43a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-626fea44-ce20-4247-81b6-a7efd31eb43a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-626fea44-ce20-4247-81b6-a7efd31eb43a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-626fea44-ce20-4247-81b6-a7efd31eb43a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Pred  Actual\n",
              "0   1.0     1.0\n",
              "1   1.0     1.0\n",
              "2   1.0     0.0\n",
              "3   1.0     1.0\n",
              "4   0.0     0.0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pred = pd.concat([pd.DataFrame(np.round(predictions)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "7-E88MzM1laI",
        "outputId": "fb10a4fc-1380-41d5-bc7b-97065fac1d79"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.5192307692307693\n",
            "MCC = 0.03559695278000926\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF9f31Rv47Wi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFk4in4P5rN3"
      },
      "source": [
        "<center><h3>Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "stqDtJn65uWa"
      },
      "outputs": [],
      "source": [
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model3 = build_model(input_shape, head_size=256, num_heads=8, ff_dim=8, num_transformer_blocks=8, mlp_units=[128], \n",
        "                    mlp_dropout=0.4,dropout=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Fk-msh5-_E",
        "outputId": "613eb970-ee47-4c80-f64b-2651303bf2fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7fc2dad4db90>>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "QuTv8hWq6Ax6"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model3.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model3.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep06gZMQ6HOE",
        "outputId": "c234709b-bbc2-48c0-c346-2774248ccc71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - ETA: 0s - loss: 4.7775 - accuracy: 0.4908\n",
            "Epoch 1: val_accuracy improved from -inf to 0.50000, saving model to clas_logs\\model3.hdf5\n",
            "46/46 [==============================] - 11s 68ms/step - loss: 4.7775 - accuracy: 0.4908 - val_loss: 0.7977 - val_accuracy: 0.5000\n",
            "Epoch 2/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.4495 - accuracy: 0.5271\n",
            "Epoch 2: val_accuracy improved from 0.50000 to 0.51496, saving model to clas_logs\\model3.hdf5\n",
            "46/46 [==============================] - 2s 48ms/step - loss: 6.4299 - accuracy: 0.5291 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 3/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0710 - accuracy: 0.5333\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 7.0424 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 4/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0449 - accuracy: 0.5375\n",
            "Epoch 4: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 7.0167 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 5/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9618 - accuracy: 0.5424\n",
            "Epoch 5: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9454 - accuracy: 0.5435 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 6/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.6963 - accuracy: 0.5479\n",
            "Epoch 6: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.6632 - accuracy: 0.5496 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 7/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.2980 - accuracy: 0.4903\n",
            "Epoch 7: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.3551 - accuracy: 0.4880 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 8/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.8855 - accuracy: 0.4681\n",
            "Epoch 8: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 7.9205 - accuracy: 0.4654 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 9/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.2376 - accuracy: 0.5133\n",
            "Epoch 9: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 7.2376 - accuracy: 0.5133 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 10/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0518 - accuracy: 0.5312\n",
            "Epoch 10: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 48ms/step - loss: 7.0235 - accuracy: 0.5332 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 11/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.1779 - accuracy: 0.5264\n",
            "Epoch 11: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 7.1478 - accuracy: 0.5284 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 12/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 12: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 13/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 13: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 14/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 14: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 15/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 15: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 16/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 16: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 17/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 17: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 18/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 18: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 48ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 19/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 19: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 20/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 20: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 58ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 21/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 21: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 22/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 22: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 23/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 23: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 24/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 24: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 25/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 25: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 26/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 26: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 27/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 27: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 28/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 28: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 29: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 30: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 31: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 32: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 33: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 34: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 35: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 36: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 37: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 38: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 39: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 40: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 41: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 42: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 43: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 44: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 45: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 46: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 47/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 47: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 48: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 49: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 50: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 51/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 51: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 52/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 52: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 53: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 54: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 55: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 56: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 57: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 58: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 59: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 60: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 61: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 62: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 63: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 64: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 65: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 66/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 66: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 67: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 68: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 69: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 70: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 71: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 72: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 73: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 74: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 75: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 76: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 77: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 78: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 79: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 80: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 81: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 82: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 83: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 84: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 85: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 86: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 87: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 88: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 89: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 90: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 91: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 92: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 61ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 93: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 94: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 95: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 96: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 97: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 98: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 99: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 100: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 101: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 102: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 103/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 103: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 104: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 105: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 106/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 106: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 107: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 108: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 109: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 110: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 111: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 113: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 114: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 116: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 117: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 118: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0317 - accuracy: 0.5389\n",
            "Epoch 119: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 120: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 121: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 122: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 123: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 124: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 125: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 126: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 127: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 128: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 129: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 130: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0317 - accuracy: 0.5389\n",
            "Epoch 131: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 133: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 134: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 135: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 136: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 137: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 138: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 139: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0214 - accuracy: 0.5396\n",
            "Epoch 140: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9935 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 142: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 143: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 144: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 145: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 146: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 147: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 148: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 150: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 151/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 151: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 152/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 152: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 153/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 153: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 154/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 154: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 155/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 155: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 156/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 156: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 157/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 157: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 158/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 158: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 159/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 159: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 160/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 160: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 161/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 161: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 162/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 162: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 163/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 163: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 164/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 164: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 165/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 165: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 166/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 166: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 167/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 167: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 168/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 168: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 169/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 169: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 170/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 170: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 171/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 171: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 172/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 172: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 173/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 173: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 174/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 174: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 175/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 175: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 176/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 176: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 177/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 177: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 178/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 178: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 179/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 179: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 180/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 180: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 181/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 181: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 182/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 182: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 183/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 183: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 184/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 184: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 185/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 185: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 186/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 186: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 187/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 187: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 188/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 188: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 189/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 189: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 190/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 190: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 191/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 191: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 192/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 192: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 193/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 193: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 194/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 194: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 195/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 195: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 196/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 196: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 197/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 197: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 198/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 198: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 199/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 199: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 200/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 200: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 201/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 201: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 202/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 202: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 203/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 203: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 204/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 204: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 205/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 205: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 206/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 206: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 207/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 207: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 208/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 208: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 209/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 209: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 210/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 210: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 211/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 211: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 212/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 212: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 213/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 213: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 214/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 214: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 215/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 215: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 216/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 216: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 217/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 217: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 218/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 218: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 219/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 219: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 220/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 220: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 221/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 221: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 222/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 222: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 223/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 223: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 224/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 224: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 225/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 225: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 226/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 226: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 227/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 227: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 228/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 228: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 229/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 229: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 230/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 230: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 231/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 231: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 232/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 232: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 233/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 233: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 234/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 234: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 235/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 235: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 236/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 236: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 237/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 237: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 238/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 238: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 239/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 239: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 240/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 240: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 241/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 241: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 242/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 242: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 243/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 243: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 244/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 244: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 245/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 245: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 246/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 246: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 247/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 247: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 248/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 248: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 249/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 249: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 250/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 250: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 251/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 251: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 252/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 252: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 253/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 253: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 254/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 254: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 255/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 255: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 256/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 256: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 257/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 257: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 258/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 258: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 259/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 259: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 260/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 260: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 261/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 261: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 262/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 262: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 263/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 263: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 264/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 264: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 265/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 265: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 266/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 266: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 267/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 267: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 268/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 268: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 269/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 269: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 270/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 270: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 271/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 271: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 272/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 272: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 273/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 273: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 274/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 274: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 275/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 275: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 276/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 276: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 277/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 277: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 278/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 278: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 279/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 279: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 280/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 280: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 281/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 281: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 282/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 282: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 283/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 283: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 284/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 284: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 285/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 285: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 286/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 286: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 287/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 287: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 288/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 288: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 289/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 289: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 290/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 290: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 291/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 291: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 292/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 292: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 293/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 293: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 294/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 294: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 295/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 295: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 296/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 296: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 297/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 297: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 298/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 298: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 299/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 299: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 300/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 300: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 301/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 301: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 302/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 302: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 303/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 303: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 304/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 304: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 305/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 305: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 306/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 306: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 307/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 307: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 308/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 308: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 309/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 309: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 310/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 310: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 311/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 311: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 312/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 312: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 313/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 313: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 314/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 314: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 315/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 315: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 316/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 316: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 317/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 317: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 318/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 318: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 319/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 319: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 320/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 320: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 321/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 321: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 322/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 322: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 323/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 323: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 324/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 324: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 325/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 325: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 326/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 326: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 327/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 327: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 328/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 328: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 329/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 329: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 330/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 330: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 331/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 331: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 332/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 332: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 333/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 333: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 334/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 334: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 335/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 335: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 336/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 336: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 337/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 337: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 338/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 338: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 339/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 339: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 340/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 340: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 341/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 341: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 342/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 342: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 343/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 343: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 344/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 344: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 345/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 345: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 346/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 346: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 347/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 347: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 348/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 348: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 349/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 349: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 350/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 350: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 351/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 351: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 352/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 352: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 353/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 353: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 354/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 354: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 355/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 355: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 356/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 356: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 357/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 357: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 358/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 358: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 359/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 359: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 360/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 360: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 361/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 361: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 362/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 362: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 363/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 363: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 364/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 364: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 365/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 365: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 366/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 366: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 367/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 367: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 368/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 368: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 369/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 369: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 370/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 370: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 371/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 371: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 372/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 372: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 373/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 373: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 374/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 374: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 375/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 375: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 376/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 376: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 377/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 377: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 378/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 378: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 379/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 379: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 380/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 380: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 381/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 381: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 382/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 382: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 383/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 383: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 384/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 384: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 385/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 385: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 386/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 386: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 387/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 387: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 388/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 388: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 389/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 389: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 390/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 390: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 391/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 391: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 392/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 392: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 393/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 393: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 394/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 394: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 395/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 395: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 396/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 396: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 397/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 397: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 398/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 398: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 399/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 399: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 400/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 400: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 401/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 401: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 402/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 402: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 403/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 403: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 404/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 404: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 405/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 405: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 406/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 406: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 407/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 407: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 408/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 408: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 409/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 409: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 410/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 410: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 411/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 411: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 412/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 412: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 413/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 413: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 414/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 414: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 415/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 415: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 416/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 416: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 417/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 417: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 418/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 418: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 419/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 419: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 420/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 420: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 421/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 421: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 422/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 422: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 423/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 423: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 424/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 424: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 425/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 425: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 426/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 426: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 427/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 427: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 428/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 428: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 429/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 429: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 430/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 430: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 431/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 431: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 432/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 432: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 433/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 433: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 434/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 434: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 435/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 435: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 436/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 436: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 437/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 437: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 438/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 438: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 439/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 439: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 440/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 440: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 441/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 441: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 442/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 442: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 443/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 443: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 444/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 444: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 445/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 445: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 446/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 446: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 447/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 447: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 448/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 448: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 449/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 449: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 450/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 450: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 451/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 451: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 452/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 452: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 453/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 453: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 454/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 454: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 455/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 455: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 456/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 456: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 457/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 457: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 458/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 458: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 459/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 459: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 460/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 460: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 461/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 461: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 462/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 462: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 463/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 463: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 464/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 464: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 465/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 465: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 466/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 466: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 467/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 467: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 468/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 468: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 469/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 469: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 470/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 470: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 471/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 471: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 472/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 472: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 473/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 473: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 474/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 474: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 475/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 475: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 476/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 476: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 47ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 477/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 477: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 478/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 478: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 479/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 479: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 480/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 480: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 481/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 481: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 482/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 482: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 483/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 483: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 484/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 484: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 485/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 485: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 486/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 486: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 487/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 487: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 488/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 488: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 489/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 489: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 490/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 490: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 491/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 491: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 492/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 492: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 493/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 493: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 494/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 494: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 495/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 495: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 496/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 496: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 497/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 497: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 498/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 498: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 499/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 499: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 500/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 500: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "# history = model1.fit()\n",
        "history = model3.fit_generator(train_generator, epochs=500, validation_data=test_generator, shuffle=False, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "auoO6h9F6K0T",
        "outputId": "cd64dd1d-58a7-4e62-a650-a9eb78829b3c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plotHist(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "Nqpa74tl6PhV",
        "outputId": "21cec697-9443-4522-c2a0-ca3ab3702062"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plotHist(history, \"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Hb2S_MNt6Rt0"
      },
      "outputs": [],
      "source": [
        "model3 = tf.keras.models.load_model(\"/content/clas_logs\\model3.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S1MNfwW6VzH",
        "outputId": "0630ecfb-3153-4d1a-8df3-89effe9733ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model3.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3aPRVD_6Xgp",
        "outputId": "f2fa5244-f19b-43ac-8da1-ff57d9f43f39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[10.429511 ],\n",
              "       [10.474612 ],\n",
              "       [10.441558 ],\n",
              "       [10.397116 ],\n",
              "       [10.456991 ],\n",
              "       [10.458649 ],\n",
              "       [10.439552 ],\n",
              "       [10.451434 ],\n",
              "       [10.518612 ],\n",
              "       [10.537264 ],\n",
              "       [10.448792 ],\n",
              "       [10.443691 ],\n",
              "       [10.452982 ],\n",
              "       [10.541955 ],\n",
              "       [10.503847 ],\n",
              "       [10.491273 ],\n",
              "       [10.510905 ],\n",
              "       [10.531432 ],\n",
              "       [10.585195 ],\n",
              "       [10.431409 ],\n",
              "       [10.420238 ],\n",
              "       [10.445921 ],\n",
              "       [10.40034  ],\n",
              "       [10.41516  ],\n",
              "       [10.434122 ],\n",
              "       [10.389135 ],\n",
              "       [10.351183 ],\n",
              "       [10.333728 ],\n",
              "       [10.302179 ],\n",
              "       [10.337588 ],\n",
              "       [10.420117 ],\n",
              "       [10.32033  ],\n",
              "       [10.400866 ],\n",
              "       [10.31154  ],\n",
              "       [10.286184 ],\n",
              "       [10.400484 ],\n",
              "       [10.261574 ],\n",
              "       [10.262381 ],\n",
              "       [10.237546 ],\n",
              "       [10.286616 ],\n",
              "       [10.385968 ],\n",
              "       [10.286502 ],\n",
              "       [10.293649 ],\n",
              "       [10.341728 ],\n",
              "       [10.325162 ],\n",
              "       [10.297115 ],\n",
              "       [10.223784 ],\n",
              "       [10.176379 ],\n",
              "       [10.112492 ],\n",
              "       [10.043747 ],\n",
              "       [10.043611 ],\n",
              "       [10.157088 ],\n",
              "       [10.167693 ],\n",
              "       [10.108664 ],\n",
              "       [10.221278 ],\n",
              "       [10.197756 ],\n",
              "       [10.146232 ],\n",
              "       [10.050096 ],\n",
              "       [10.112408 ],\n",
              "       [10.23767  ],\n",
              "       [10.109336 ],\n",
              "       [10.10065  ],\n",
              "       [10.0979805],\n",
              "       [10.211674 ],\n",
              "       [10.16496  ],\n",
              "       [10.095696 ],\n",
              "       [10.233591 ],\n",
              "       [10.260932 ],\n",
              "       [10.330951 ],\n",
              "       [10.171392 ],\n",
              "       [10.097062 ],\n",
              "       [10.172228 ],\n",
              "       [10.139153 ],\n",
              "       [10.104211 ],\n",
              "       [10.010016 ],\n",
              "       [10.010695 ],\n",
              "       [10.070821 ],\n",
              "       [10.030375 ],\n",
              "       [10.069042 ],\n",
              "       [10.078649 ],\n",
              "       [10.104434 ],\n",
              "       [10.141305 ],\n",
              "       [10.109614 ],\n",
              "       [10.182565 ],\n",
              "       [10.191086 ],\n",
              "       [10.166881 ],\n",
              "       [10.211122 ],\n",
              "       [10.137983 ],\n",
              "       [10.136283 ],\n",
              "       [10.094774 ],\n",
              "       [10.148798 ],\n",
              "       [10.112414 ],\n",
              "       [10.08746  ],\n",
              "       [10.07237  ],\n",
              "       [10.099655 ],\n",
              "       [10.062379 ],\n",
              "       [10.084852 ],\n",
              "       [10.0806265],\n",
              "       [10.124461 ],\n",
              "       [10.038789 ],\n",
              "       [10.126526 ],\n",
              "       [10.20105  ],\n",
              "       [10.257752 ],\n",
              "       [10.249996 ],\n",
              "       [10.232194 ],\n",
              "       [10.304673 ],\n",
              "       [10.3067045],\n",
              "       [10.24282  ],\n",
              "       [10.320939 ],\n",
              "       [10.389162 ],\n",
              "       [10.349577 ],\n",
              "       [10.281271 ],\n",
              "       [10.313543 ],\n",
              "       [10.264011 ],\n",
              "       [10.332071 ],\n",
              "       [10.198053 ],\n",
              "       [10.198861 ],\n",
              "       [10.241371 ],\n",
              "       [10.209381 ],\n",
              "       [10.268849 ],\n",
              "       [10.32776  ],\n",
              "       [10.342976 ],\n",
              "       [10.325405 ],\n",
              "       [10.279925 ],\n",
              "       [10.330707 ],\n",
              "       [10.237478 ],\n",
              "       [10.27673  ],\n",
              "       [10.267578 ],\n",
              "       [10.228777 ],\n",
              "       [10.27946  ],\n",
              "       [10.198875 ],\n",
              "       [10.163641 ],\n",
              "       [10.251527 ],\n",
              "       [10.183951 ],\n",
              "       [10.189638 ],\n",
              "       [10.135117 ],\n",
              "       [10.110697 ],\n",
              "       [10.134738 ],\n",
              "       [10.1403675],\n",
              "       [10.157887 ],\n",
              "       [10.211825 ],\n",
              "       [10.127547 ],\n",
              "       [10.020684 ],\n",
              "       [ 9.89403  ],\n",
              "       [ 9.924295 ],\n",
              "       [ 9.926982 ],\n",
              "       [ 9.811173 ],\n",
              "       [ 9.779314 ],\n",
              "       [ 9.767547 ],\n",
              "       [ 9.84825  ],\n",
              "       [ 9.827414 ],\n",
              "       [ 9.730127 ],\n",
              "       [ 9.828442 ],\n",
              "       [ 9.871357 ],\n",
              "       [ 9.7664995],\n",
              "       [ 9.792401 ],\n",
              "       [ 9.74492  ],\n",
              "       [ 9.895081 ],\n",
              "       [ 9.865228 ],\n",
              "       [ 9.822425 ],\n",
              "       [ 9.89705  ],\n",
              "       [ 9.864779 ],\n",
              "       [ 9.766275 ],\n",
              "       [ 9.76576  ],\n",
              "       [ 9.783758 ],\n",
              "       [ 9.840837 ],\n",
              "       [ 9.820246 ],\n",
              "       [ 9.825612 ],\n",
              "       [ 9.789139 ],\n",
              "       [ 9.831462 ],\n",
              "       [ 9.743413 ],\n",
              "       [ 9.823461 ],\n",
              "       [ 9.934207 ],\n",
              "       [10.045003 ],\n",
              "       [ 9.869241 ],\n",
              "       [ 9.897479 ],\n",
              "       [10.114786 ],\n",
              "       [10.156155 ],\n",
              "       [10.058788 ],\n",
              "       [10.0319395],\n",
              "       [10.065225 ],\n",
              "       [10.074544 ],\n",
              "       [ 9.979571 ],\n",
              "       [10.039292 ],\n",
              "       [10.100542 ],\n",
              "       [10.100599 ],\n",
              "       [ 9.985874 ],\n",
              "       [ 9.942044 ],\n",
              "       [ 9.975682 ],\n",
              "       [ 9.98168  ],\n",
              "       [ 9.984939 ],\n",
              "       [ 9.943333 ],\n",
              "       [10.016519 ],\n",
              "       [10.064257 ],\n",
              "       [ 9.93329  ],\n",
              "       [ 9.961968 ],\n",
              "       [10.003658 ],\n",
              "       [ 9.954046 ],\n",
              "       [ 9.870848 ],\n",
              "       [ 9.845936 ],\n",
              "       [ 9.894802 ],\n",
              "       [ 9.878711 ],\n",
              "       [ 9.812194 ],\n",
              "       [ 9.866954 ],\n",
              "       [ 9.921296 ],\n",
              "       [ 9.924    ],\n",
              "       [ 9.846459 ],\n",
              "       [ 9.882204 ],\n",
              "       [ 9.900153 ],\n",
              "       [ 9.889237 ],\n",
              "       [ 9.889922 ],\n",
              "       [ 9.893072 ],\n",
              "       [ 9.930097 ],\n",
              "       [ 9.874615 ],\n",
              "       [ 9.874745 ],\n",
              "       [ 9.979385 ],\n",
              "       [ 9.93553  ],\n",
              "       [ 9.968328 ],\n",
              "       [ 9.969413 ],\n",
              "       [ 9.9842415],\n",
              "       [10.155971 ],\n",
              "       [10.089003 ],\n",
              "       [10.076437 ],\n",
              "       [10.153371 ],\n",
              "       [10.126591 ],\n",
              "       [10.157574 ],\n",
              "       [10.156338 ],\n",
              "       [10.224211 ],\n",
              "       [10.349544 ],\n",
              "       [10.233383 ],\n",
              "       [10.160472 ],\n",
              "       [10.226192 ],\n",
              "       [10.295741 ],\n",
              "       [10.353137 ],\n",
              "       [10.2787   ],\n",
              "       [10.228435 ],\n",
              "       [10.246376 ],\n",
              "       [10.167712 ],\n",
              "       [10.134749 ],\n",
              "       [10.187093 ],\n",
              "       [10.248017 ],\n",
              "       [10.22525  ],\n",
              "       [10.216397 ],\n",
              "       [10.113759 ],\n",
              "       [10.15392  ],\n",
              "       [10.233775 ],\n",
              "       [10.160619 ],\n",
              "       [10.179698 ],\n",
              "       [10.169443 ],\n",
              "       [10.040397 ],\n",
              "       [ 9.968078 ],\n",
              "       [ 9.90137  ],\n",
              "       [10.009286 ],\n",
              "       [10.106499 ],\n",
              "       [10.118152 ],\n",
              "       [10.119765 ],\n",
              "       [10.134515 ],\n",
              "       [10.162713 ],\n",
              "       [10.17818  ],\n",
              "       [10.216809 ],\n",
              "       [10.29229  ],\n",
              "       [10.408725 ],\n",
              "       [10.404396 ],\n",
              "       [10.31674  ],\n",
              "       [10.38594  ],\n",
              "       [10.537044 ],\n",
              "       [10.54376  ],\n",
              "       [10.58769  ],\n",
              "       [10.55472  ],\n",
              "       [10.559305 ],\n",
              "       [10.671439 ],\n",
              "       [10.551208 ],\n",
              "       [10.59766  ],\n",
              "       [10.718813 ],\n",
              "       [10.787673 ],\n",
              "       [10.757206 ],\n",
              "       [10.748961 ],\n",
              "       [10.917657 ],\n",
              "       [10.895363 ],\n",
              "       [10.989702 ],\n",
              "       [11.040524 ],\n",
              "       [11.07714  ],\n",
              "       [11.132204 ],\n",
              "       [11.075275 ],\n",
              "       [11.026696 ],\n",
              "       [11.077088 ],\n",
              "       [11.019511 ],\n",
              "       [10.968563 ],\n",
              "       [10.9961195],\n",
              "       [10.992856 ],\n",
              "       [10.987791 ],\n",
              "       [10.946627 ],\n",
              "       [10.929107 ],\n",
              "       [10.932779 ],\n",
              "       [10.9637   ],\n",
              "       [10.874795 ],\n",
              "       [10.838238 ],\n",
              "       [10.910137 ],\n",
              "       [10.759192 ],\n",
              "       [10.682478 ],\n",
              "       [10.71883  ],\n",
              "       [10.748644 ],\n",
              "       [10.736145 ],\n",
              "       [10.640574 ],\n",
              "       [10.63177  ],\n",
              "       [10.621361 ],\n",
              "       [10.518305 ],\n",
              "       [10.420408 ],\n",
              "       [10.37519  ],\n",
              "       [10.335812 ],\n",
              "       [10.321769 ],\n",
              "       [10.2319   ],\n",
              "       [10.263622 ],\n",
              "       [10.303971 ],\n",
              "       [10.255225 ],\n",
              "       [10.260768 ],\n",
              "       [10.286456 ],\n",
              "       [10.337752 ],\n",
              "       [10.334976 ],\n",
              "       [10.164291 ],\n",
              "       [10.229179 ],\n",
              "       [10.2301   ],\n",
              "       [10.178283 ],\n",
              "       [10.2245245],\n",
              "       [10.184438 ],\n",
              "       [10.154253 ],\n",
              "       [10.181538 ],\n",
              "       [10.1943655],\n",
              "       [10.180431 ],\n",
              "       [10.229229 ],\n",
              "       [10.249925 ],\n",
              "       [10.250121 ],\n",
              "       [10.225498 ],\n",
              "       [10.217313 ],\n",
              "       [10.286861 ],\n",
              "       [10.35028  ],\n",
              "       [10.39128  ],\n",
              "       [10.377728 ],\n",
              "       [10.387158 ],\n",
              "       [10.40959  ],\n",
              "       [10.3768425],\n",
              "       [10.342205 ],\n",
              "       [10.281696 ],\n",
              "       [10.389395 ],\n",
              "       [10.341368 ],\n",
              "       [10.303325 ],\n",
              "       [10.266039 ],\n",
              "       [10.367343 ],\n",
              "       [10.405117 ],\n",
              "       [10.375197 ],\n",
              "       [10.489722 ],\n",
              "       [10.519246 ],\n",
              "       [10.514239 ],\n",
              "       [10.504888 ],\n",
              "       [10.618923 ],\n",
              "       [10.761264 ],\n",
              "       [10.744178 ],\n",
              "       [10.677192 ],\n",
              "       [10.713744 ],\n",
              "       [10.816185 ],\n",
              "       [10.760311 ],\n",
              "       [10.749496 ],\n",
              "       [10.7843075],\n",
              "       [10.87685  ],\n",
              "       [10.8556385],\n",
              "       [10.680761 ],\n",
              "       [10.770637 ],\n",
              "       [10.802392 ],\n",
              "       [10.786677 ],\n",
              "       [10.754043 ],\n",
              "       [10.864293 ],\n",
              "       [10.88885  ],\n",
              "       [10.908118 ],\n",
              "       [10.827545 ],\n",
              "       [10.811882 ],\n",
              "       [10.924934 ],\n",
              "       [10.892568 ],\n",
              "       [10.812117 ],\n",
              "       [10.920609 ],\n",
              "       [10.891646 ],\n",
              "       [10.788961 ],\n",
              "       [10.725326 ],\n",
              "       [10.76571  ],\n",
              "       [10.820244 ],\n",
              "       [10.815676 ],\n",
              "       [10.73766  ],\n",
              "       [10.660806 ],\n",
              "       [10.74582  ],\n",
              "       [10.6137295],\n",
              "       [10.595035 ],\n",
              "       [10.641832 ],\n",
              "       [10.748082 ],\n",
              "       [10.788638 ],\n",
              "       [10.753049 ],\n",
              "       [10.711412 ],\n",
              "       [10.734842 ],\n",
              "       [10.705835 ],\n",
              "       [10.675177 ],\n",
              "       [10.747292 ],\n",
              "       [10.804535 ],\n",
              "       [10.681365 ],\n",
              "       [10.882148 ],\n",
              "       [10.795166 ],\n",
              "       [10.796109 ],\n",
              "       [10.750022 ],\n",
              "       [10.715319 ],\n",
              "       [10.729651 ],\n",
              "       [10.705157 ],\n",
              "       [10.568562 ],\n",
              "       [10.604165 ],\n",
              "       [10.644115 ],\n",
              "       [10.598536 ],\n",
              "       [10.669208 ],\n",
              "       [10.602829 ],\n",
              "       [10.556253 ],\n",
              "       [10.576602 ],\n",
              "       [10.504821 ],\n",
              "       [10.4237995],\n",
              "       [10.457189 ],\n",
              "       [10.446626 ],\n",
              "       [10.409126 ],\n",
              "       [10.371737 ],\n",
              "       [10.314794 ],\n",
              "       [10.232921 ],\n",
              "       [10.314446 ],\n",
              "       [10.196324 ],\n",
              "       [10.201537 ],\n",
              "       [10.2924185],\n",
              "       [10.246263 ],\n",
              "       [10.126767 ],\n",
              "       [10.133509 ],\n",
              "       [10.086868 ],\n",
              "       [10.18595  ],\n",
              "       [10.063488 ],\n",
              "       [10.086471 ],\n",
              "       [10.085009 ],\n",
              "       [10.08015  ],\n",
              "       [10.035562 ],\n",
              "       [10.105434 ],\n",
              "       [10.201286 ],\n",
              "       [10.163424 ],\n",
              "       [10.175077 ],\n",
              "       [10.132663 ],\n",
              "       [10.066082 ],\n",
              "       [10.118455 ],\n",
              "       [10.064245 ],\n",
              "       [10.09889  ],\n",
              "       [10.1131115],\n",
              "       [10.092904 ],\n",
              "       [10.135558 ],\n",
              "       [10.144078 ],\n",
              "       [10.030759 ],\n",
              "       [10.100526 ],\n",
              "       [10.04726  ],\n",
              "       [10.079374 ],\n",
              "       [10.037119 ],\n",
              "       [10.060304 ],\n",
              "       [10.091105 ],\n",
              "       [10.107254 ],\n",
              "       [10.157044 ],\n",
              "       [10.14095  ],\n",
              "       [10.1745   ],\n",
              "       [10.216986 ],\n",
              "       [10.13891  ],\n",
              "       [10.190933 ],\n",
              "       [10.243309 ],\n",
              "       [10.296612 ],\n",
              "       [10.312558 ]], dtype=float32)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKMsjHqi6ZMH",
        "outputId": "46e101cd-f283-4199-9588-199c35137d93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(468, 1)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6b1S8gc6c8T",
        "outputId": "4d52ec25-efbc-4116-f556-beb36236e7df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(468,)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test[:,1][win_len:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c-vz1yNJ6efg",
        "outputId": "38dff5a0-6cf1-4a34-8e48-9e26da9e1d2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-51ac6de6-e690-44da-aae4-0a8517150af0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51ac6de6-e690-44da-aae4-0a8517150af0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51ac6de6-e690-44da-aae4-0a8517150af0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51ac6de6-e690-44da-aae4-0a8517150af0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Pred  Actual\n",
              "0  10.0     1.0\n",
              "1  10.0     1.0\n",
              "2  10.0     0.0\n",
              "3  10.0     1.0\n",
              "4  10.0     0.0"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pred = pd.concat([pd.DataFrame(np.round(predictions)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "3rdaRAhL6giP",
        "outputId": "2c51da9e-6307-4f6e-eeec-919556b08cc0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1.0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-50017a17bc17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-4f0006d59e97>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppp4ZFGK6ivQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e4b444f792fbff6b064e16dd46d58b2c8126eac8e1e261c7b4c56b3fda4323e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
